"use strict";(globalThis.webpackChunkphysical_ai_textbook=globalThis.webpackChunkphysical_ai_textbook||[]).push([[128],{7302:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>m,frontMatter:()=>o,metadata:()=>r,toc:()=>c});var t=s(4848),i=s(8453);const o={id:"chapter15",sidebar_position:3,title:"Capstone Project \u2013 Autonomous Humanoid from Spoken Command"},l="Chapter 15: Capstone Project \u2013 Autonomous Humanoid from Spoken Command",r={id:"module4/chapter15",title:"Capstone Project \u2013 Autonomous Humanoid from Spoken Command",description:"Learning Objectives",source:"@site/docs/module4/chapter15.mdx",sourceDirName:"module4",slug:"/module4/chapter15",permalink:"/physical-ai-textbook/docs/module4/chapter15",draft:!1,unlisted:!1,editUrl:"https://github.com/your-github-username/physical-ai-textbook/tree/main/docs/module4/chapter15.mdx",tags:[],version:"current",sidebarPosition:3,frontMatter:{id:"chapter15",sidebar_position:3,title:"Capstone Project \u2013 Autonomous Humanoid from Spoken Command"},sidebar:"tutorialSidebar",previous:{title:"From Voice \u2192 Plan \u2192 Action",permalink:"/physical-ai-textbook/docs/module4/chapter14"},next:{title:"Physical AI & Humanoid Robotics Textbook - Project Summary",permalink:"/physical-ai-textbook/docs/final_review/project_summary"}},a={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"15.1 Introduction to the Capstone Project",id:"151-introduction-to-the-capstone-project",level:2},{value:"15.2 Project Requirements and Specifications",id:"152-project-requirements-and-specifications",level:2},{value:"15.2.1 Functional Requirements",id:"1521-functional-requirements",level:3},{value:"15.2.2 Non-Functional Requirements",id:"1522-non-functional-requirements",level:3},{value:"15.2.3 Success Criteria",id:"1523-success-criteria",level:3},{value:"15.3 System Architecture",id:"153-system-architecture",level:2},{value:"15.3.1 High-Level Architecture",id:"1531-high-level-architecture",level:3},{value:"15.3.2 Component Integration",id:"1532-component-integration",level:3},{value:"Voice Processing Module",id:"voice-processing-module",level:4},{value:"Perception Module",id:"perception-module",level:4},{value:"Planning Module",id:"planning-module",level:4},{value:"Execution Module",id:"execution-module",level:4},{value:"15.4 Implementation Strategy",id:"154-implementation-strategy",level:2},{value:"15.4.1 Development Phases",id:"1541-development-phases",level:3},{value:"Phase 1: Component Integration",id:"phase-1-component-integration",level:4},{value:"Phase 2: System Integration",id:"phase-2-system-integration",level:4},{value:"Phase 3: Real-World Deployment",id:"phase-3-real-world-deployment",level:4},{value:"15.4.2 Testing Strategy",id:"1542-testing-strategy",level:3},{value:"Unit Testing",id:"unit-testing",level:4},{value:"Integration Testing",id:"integration-testing",level:4},{value:"15.5 Safety and Risk Management",id:"155-safety-and-risk-management",level:2},{value:"15.5.1 Safety Architecture",id:"1551-safety-architecture",level:3},{value:"Hardware Safety",id:"hardware-safety",level:4},{value:"Software Safety",id:"software-safety",level:4},{value:"Operational Safety",id:"operational-safety",level:4},{value:"15.5.2 Risk Assessment",id:"1552-risk-assessment",level:3},{value:"15.6 Evaluation and Assessment",id:"156-evaluation-and-assessment",level:2},{value:"15.6.1 Evaluation Metrics",id:"1561-evaluation-metrics",level:3},{value:"Primary Metrics",id:"primary-metrics",level:4},{value:"Secondary Metrics",id:"secondary-metrics",level:4},{value:"15.6.2 Evaluation Protocol",id:"1562-evaluation-protocol",level:3},{value:"Test Environment Setup",id:"test-environment-setup",level:4},{value:"Test Scenarios",id:"test-scenarios",level:4},{value:"Data Collection",id:"data-collection",level:4},{value:"15.6.3 Assessment Rubric",id:"1563-assessment-rubric",level:3},{value:"15.7 Implementation Guide",id:"157-implementation-guide",level:2},{value:"15.7.1 Code Structure",id:"1571-code-structure",level:3},{value:"15.7.2 Main System Integration",id:"1572-main-system-integration",level:3},{value:"15.7.3 Launch File",id:"1573-launch-file",level:3},{value:"15.8 Troubleshooting and Debugging",id:"158-troubleshooting-and-debugging",level:2},{value:"15.8.1 Common Issues",id:"1581-common-issues",level:3},{value:"Integration Issues",id:"integration-issues",level:4},{value:"Performance Issues",id:"performance-issues",level:4},{value:"Safety Issues",id:"safety-issues",level:4},{value:"15.8.2 Debugging Tools",id:"1582-debugging-tools",level:3},{value:"ROS 2 Tools",id:"ros-2-tools",level:4},{value:"Custom Debugging",id:"custom-debugging",level:4},{value:"15.9 Conclusion",id:"159-conclusion",level:2},{value:"Exercises",id:"exercises",level:2},{value:"References",id:"references",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"chapter-15-capstone-project--autonomous-humanoid-from-spoken-command",children:"Chapter 15: Capstone Project \u2013 Autonomous Humanoid from Spoken Command"}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Integrate all components learned throughout the textbook into a complete autonomous humanoid system"}),"\n",(0,t.jsx)(n.li,{children:"Implement an end-to-end system that responds to spoken commands with complex robotic behaviors"}),"\n",(0,t.jsx)(n.li,{children:"Design and execute a comprehensive evaluation of the complete system"}),"\n",(0,t.jsx)(n.li,{children:"Identify and address integration challenges between different subsystems"}),"\n",(0,t.jsx)(n.li,{children:"Document and present their capstone project results"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"151-introduction-to-the-capstone-project",children:"15.1 Introduction to the Capstone Project"}),"\n",(0,t.jsx)(n.p,{children:"The capstone project represents the culmination of all knowledge and skills acquired throughout this textbook. Students will implement a complete autonomous humanoid system capable of understanding and executing spoken commands in real-world environments. This project integrates:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Physical AI & Embodied Intelligence"}),": Understanding how the robot's physical form affects its intelligence and behavior"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Architecture"}),": Using the robot operating system for communication and coordination"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Simulation & Control"}),": Leveraging simulation for development and testing before real-world deployment"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"NVIDIA Isaac Platform"}),": Utilizing hardware-accelerated perception and navigation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision-Language-Action Models"}),": Enabling natural human-robot interaction through spoken commands"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["The primary challenge for this capstone is: ",(0,t.jsx)(n.strong,{children:'"Pick up the red cup"'})," - a task that requires the integration of perception, planning, manipulation, and execution in response to a simple spoken command."]}),"\n",(0,t.jsx)(n.h2,{id:"152-project-requirements-and-specifications",children:"15.2 Project Requirements and Specifications"}),"\n",(0,t.jsx)(n.h3,{id:"1521-functional-requirements",children:"15.2.1 Functional Requirements"}),"\n",(0,t.jsx)(n.p,{children:"The complete system must:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Voice Recognition"}),": Accurately recognize spoken commands using Whisper or similar technology"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural Language Understanding"}),": Parse commands to extract intent and relevant entities (object, location, action)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception"}),': Detect and localize the specified object (e.g., "red cup") in the environment using VSLAM and object detection']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Planning"}),": Generate a sequence of actions to approach, grasp, and manipulate the object"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Execution"}),": Safely execute the planned actions using the humanoid robot's locomotion and manipulation capabilities"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safety"}),": Implement safety checks and emergency stop functionality throughout the process"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"1522-non-functional-requirements",children:"15.2.2 Non-Functional Requirements"}),"\n",(0,t.jsx)(n.p,{children:"The system must also meet:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Performance"}),": Complete the task within 5 minutes from command to execution"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Reliability"}),": Achieve 80% success rate over 10 trials"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safety"}),": Include multiple safety layers to prevent harm to robot, environment, or humans"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robustness"}),": Handle variations in object placement, lighting, and acoustic conditions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Documentation"}),": Provide comprehensive documentation for system setup and operation"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"1523-success-criteria",children:"15.2.3 Success Criteria"}),"\n",(0,t.jsx)(n.p,{children:"A successful implementation will:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Correctly identify the specified object (accuracy > 90%)"}),"\n",(0,t.jsx)(n.li,{children:"Navigate safely to the object location"}),"\n",(0,t.jsx)(n.li,{children:"Execute grasping with 80% success rate"}),"\n",(0,t.jsx)(n.li,{children:'Complete the "pick up" action without falling or causing damage'}),"\n",(0,t.jsx)(n.li,{children:"Respond to spoken commands naturally and efficiently"}),"\n",(0,t.jsx)(n.li,{children:"Demonstrate the complete pipeline in a real-world environment"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"153-system-architecture",children:"15.3 System Architecture"}),"\n",(0,t.jsx)(n.h3,{id:"1531-high-level-architecture",children:"15.3.1 High-Level Architecture"}),"\n",(0,t.jsx)(n.p,{children:"The complete system follows a modular architecture with clear interfaces:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"[Voice Input] \u2192 [Speech Recognition] \u2192 [NLU] \u2192 [Task Planner] \u2192 [Action Executor] \u2192 [Robot Control]\n      \u2191              \u2193                    \u2193         \u2193              \u2193                \u2193\n[Environment] \u2190 [Perception] \u2190 [VSLAM] \u2190 [Fusion] \u2190 [Scheduler] \u2190 [Safety Layer] \u2190 [Robot]\n"})}),"\n",(0,t.jsx)(n.h3,{id:"1532-component-integration",children:"15.3.2 Component Integration"}),"\n",(0,t.jsx)(n.p,{children:"Each major component from previous modules integrates as follows:"}),"\n",(0,t.jsx)(n.h4,{id:"voice-processing-module",children:"Voice Processing Module"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class VoiceProcessingModule:\n    def __init__(self):\n        self.speech_recognizer = WhisperModel()\n        self.nlu_processor = NaturalLanguageProcessor()\n        self.command_validator = CommandValidator()\n\n    def process_voice_command(self, audio_input):\n        # Convert speech to text\n        text = self.speech_recognizer.transcribe(audio_input)\n\n        # Parse natural language\n        parsed_command = self.nlu_processor.parse_command(text)\n\n        # Validate command safety\n        is_safe, reason = self.command_validator.validate(parsed_command)\n\n        if is_safe:\n            return parsed_command\n        else:\n            raise UnsafeCommandError(reason)\n"})}),"\n",(0,t.jsx)(n.h4,{id:"perception-module",children:"Perception Module"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class PerceptionModule:\n    def __init__(self):\n        self.vslam_system = VSLAMSystem()\n        self.object_detector = ObjectDetectionSystem()\n        self.fusion_system = PerceptionFusionSystem()\n\n    def perceive_environment(self):\n        # Get robot pose from VSLAM\n        robot_pose = self.vslam_system.get_pose()\n\n        # Detect objects in current view\n        detections = self.object_detector.detect_objects()\n\n        # Fuse detections with robot pose\n        world_objects = self.fusion_system.fuse_detections(detections, robot_pose)\n\n        return world_objects, robot_pose\n"})}),"\n",(0,t.jsx)(n.h4,{id:"planning-module",children:"Planning Module"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class PlanningModule:\n    def __init__(self):\n        self.task_planner = TaskPlanner()\n        self.motion_planner = MotionPlanner()\n        self.safety_checker = SafetyChecker()\n\n    def plan_action_sequence(self, goal, world_state):\n        # Plan high-level task\n        task_plan = self.task_planner.plan(goal, world_state)\n\n        # Generate motion plans for each task step\n        motion_plans = []\n        for task in task_plan:\n            motion_plan = self.motion_planner.plan_motion(task, world_state)\n\n            # Check safety of motion plan\n            if not self.safety_checker.is_safe(motion_plan, world_state):\n                raise UnsafePlanError(f"Motion plan for {task} is unsafe")\n\n            motion_plans.append(motion_plan)\n\n        return motion_plans\n'})}),"\n",(0,t.jsx)(n.h4,{id:"execution-module",children:"Execution Module"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class ExecutionModule:\n    def __init__(self):\n        self.action_executor = ActionExecutor()\n        self.monitoring_system = ExecutionMonitor()\n        self.recovery_system = ErrorRecoverySystem()\n\n    def execute_plan(self, plan, world_state):\n        for i, action in enumerate(plan):\n            try:\n                # Execute action with monitoring\n                result = self.action_executor.execute_with_monitoring(\n                    action,\n                    world_state,\n                    self.monitoring_system\n                )\n\n                # Update world state based on execution result\n                world_state = self.update_world_state(world_state, action, result)\n\n            except ExecutionError as e:\n                # Attempt recovery\n                recovery_success = self.recovery_system.attempt_recovery(e, plan, i)\n\n                if not recovery_success:\n                    raise SystemFailureError(f"Recovery failed for action {i}")\n\n        return True  # Plan completed successfully\n'})}),"\n",(0,t.jsx)(n.h2,{id:"154-implementation-strategy",children:"15.4 Implementation Strategy"}),"\n",(0,t.jsx)(n.h3,{id:"1541-development-phases",children:"15.4.1 Development Phases"}),"\n",(0,t.jsx)(n.p,{children:"The implementation follows an iterative approach with the following phases:"}),"\n",(0,t.jsx)(n.h4,{id:"phase-1-component-integration",children:"Phase 1: Component Integration"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Integrate voice processing and NLU components"}),"\n",(0,t.jsx)(n.li,{children:"Connect perception pipeline to VSLAM and object detection"}),"\n",(0,t.jsx)(n.li,{children:"Implement basic task planning with simple actions"}),"\n",(0,t.jsx)(n.li,{children:"Create a simple execution system for testing"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"phase-2-system-integration",children:"Phase 2: System Integration"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Connect all modules with proper message passing"}),"\n",(0,t.jsx)(n.li,{children:"Implement safety and monitoring systems"}),"\n",(0,t.jsx)(n.li,{children:"Test on simulated environment"}),"\n",(0,t.jsx)(n.li,{children:"Debug communication and timing issues"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"phase-3-real-world-deployment",children:"Phase 3: Real-World Deployment"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Deploy on physical humanoid robot"}),"\n",(0,t.jsx)(n.li,{children:"Test with real objects and environments"}),"\n",(0,t.jsx)(n.li,{children:"Refine parameters based on real-world performance"}),"\n",(0,t.jsx)(n.li,{children:"Conduct systematic evaluation"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"1542-testing-strategy",children:"15.4.2 Testing Strategy"}),"\n",(0,t.jsx)(n.h4,{id:"unit-testing",children:"Unit Testing"}),"\n",(0,t.jsx)(n.p,{children:"Each component should be tested individually:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"import unittest\n\nclass TestVoiceProcessing(unittest.TestCase):\n    def setUp(self):\n        self.vpm = VoiceProcessingModule()\n\n    def test_command_parsing(self):\n        command = \"Pick up the red cup\"\n        parsed = self.vpm.nlu_processor.parse_command(command)\n\n        self.assertEqual(parsed['intent'], 'manipulation')\n        self.assertIn('red cup', parsed['entities']['objects'])\n\nclass TestPerceptionSystem(unittest.TestCase):\n    def setUp(self):\n        self.perception = PerceptionModule()\n\n    def test_object_detection(self):\n        # Test with known image containing objects\n        detections = self.perception.object_detector.detect_objects(test_image)\n\n        # Verify expected objects are detected\n        detected_names = [det['name'] for det in detections]\n        self.assertIn('cup', detected_names)\n"})}),"\n",(0,t.jsx)(n.h4,{id:"integration-testing",children:"Integration Testing"}),"\n",(0,t.jsx)(n.p,{children:"Test component interactions:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class TestSystemIntegration(unittest.TestCase):\n    def setUp(self):\n        self.voice_module = VoiceProcessingModule()\n        self.perception_module = PerceptionModule()\n        self.planning_module = PlanningModule()\n        self.execution_module = ExecutionModule()\n\n    def test_complete_pipeline(self):\n        # Simulate voice command\n        audio_input = self.create_test_audio("Pick up the red cup")\n\n        # Process through all modules\n        command = self.voice_module.process_voice_command(audio_input)\n        world_state = self.perception_module.perceive_environment()\n        plan = self.planning_module.plan_action_sequence(command, world_state)\n        success = self.execution_module.execute_plan(plan, world_state)\n\n        self.assertTrue(success)\n'})}),"\n",(0,t.jsx)(n.h2,{id:"155-safety-and-risk-management",children:"15.5 Safety and Risk Management"}),"\n",(0,t.jsx)(n.h3,{id:"1551-safety-architecture",children:"15.5.1 Safety Architecture"}),"\n",(0,t.jsx)(n.p,{children:"The system implements multiple layers of safety:"}),"\n",(0,t.jsx)(n.h4,{id:"hardware-safety",children:"Hardware Safety"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Emergency stop buttons accessible to operators"}),"\n",(0,t.jsx)(n.li,{children:"Joint torque limits to prevent excessive force"}),"\n",(0,t.jsx)(n.li,{children:"Collision detection and avoidance systems"}),"\n",(0,t.jsx)(n.li,{children:"Fall detection and recovery mechanisms"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"software-safety",children:"Software Safety"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Command validation to prevent unsafe actions"}),"\n",(0,t.jsx)(n.li,{children:"Motion planning with obstacle avoidance"}),"\n",(0,t.jsx)(n.li,{children:"Execution monitoring with timeout mechanisms"}),"\n",(0,t.jsx)(n.li,{children:"Graceful degradation when components fail"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"operational-safety",children:"Operational Safety"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Controlled testing environments"}),"\n",(0,t.jsx)(n.li,{children:"Human supervision during development"}),"\n",(0,t.jsx)(n.li,{children:"Clear protocols for system reset and recovery"}),"\n",(0,t.jsx)(n.li,{children:"Documentation of emergency procedures"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"1552-risk-assessment",children:"15.5.2 Risk Assessment"}),"\n",(0,t.jsx)(n.p,{children:"Identified risks and mitigation strategies:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Risk"}),(0,t.jsx)(n.th,{children:"Impact"}),(0,t.jsx)(n.th,{children:"Probability"}),(0,t.jsx)(n.th,{children:"Mitigation"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Robot falls during execution"}),(0,t.jsx)(n.td,{children:"High"}),(0,t.jsx)(n.td,{children:"Medium"}),(0,t.jsx)(n.td,{children:"Implement balance control, limit motion speeds, use safety cage during testing"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Object damage during grasping"}),(0,t.jsx)(n.td,{children:"Medium"}),(0,t.jsx)(n.td,{children:"Medium"}),(0,t.jsx)(n.td,{children:"Use force control, implement grasp force limits, use compliant grippers"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Navigation collision"}),(0,t.jsx)(n.td,{children:"High"}),(0,t.jsx)(n.td,{children:"Low"}),(0,t.jsx)(n.td,{children:"Use obstacle detection, implement safety margins, limit speed"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Voice command misinterpretation"}),(0,t.jsx)(n.td,{children:"Medium"}),(0,t.jsx)(n.td,{children:"Medium"}),(0,t.jsx)(n.td,{children:"Use confidence thresholds, implement command confirmation, allow user correction"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"System failure mid-task"}),(0,t.jsx)(n.td,{children:"Medium"}),(0,t.jsx)(n.td,{children:"Low"}),(0,t.jsx)(n.td,{children:"Implement checkpoint/recovery, use graceful degradation, provide status feedback"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"156-evaluation-and-assessment",children:"15.6 Evaluation and Assessment"}),"\n",(0,t.jsx)(n.h3,{id:"1561-evaluation-metrics",children:"15.6.1 Evaluation Metrics"}),"\n",(0,t.jsx)(n.p,{children:"The system will be evaluated using the following metrics:"}),"\n",(0,t.jsx)(n.h4,{id:"primary-metrics",children:"Primary Metrics"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Task Success Rate"}),": Percentage of times the robot successfully picks up the specified object"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Time to Completion"}),": Total time from command to successful object pickup"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Command Accuracy"}),": Percentage of correctly interpreted voice commands"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Navigation Safety"}),": Number of collisions or near-misses during navigation"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"secondary-metrics",children:"Secondary Metrics"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Perception Accuracy"}),": Accuracy of object detection and localization"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Planning Efficiency"}),": Time and quality of generated plans"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Execution Smoothness"}),": Measure of motion quality and stability"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"User Satisfaction"}),": Subjective measure of system usability"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"1562-evaluation-protocol",children:"15.6.2 Evaluation Protocol"}),"\n",(0,t.jsx)(n.h4,{id:"test-environment-setup",children:"Test Environment Setup"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Standardized test area with known objects"}),"\n",(0,t.jsx)(n.li,{children:"Fixed lighting and acoustic conditions"}),"\n",(0,t.jsx)(n.li,{children:"Calibrated camera and microphone positions"}),"\n",(0,t.jsx)(n.li,{children:"Safety equipment and emergency procedures in place"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"test-scenarios",children:"Test Scenarios"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Baseline Scenario"}),": Object in clear, unobstructed location"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Cluttered Scenario"}),": Object among other similar objects"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Challenging Lighting"}),": Low light or high contrast conditions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Acoustic Challenges"}),": Background noise or reverberation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multiple Objects"}),": Multiple similar objects requiring disambiguation"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"data-collection",children:"Data Collection"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Record all sensor data, commands, and robot states"}),"\n",(0,t.jsx)(n.li,{children:"Log all system decisions and actions"}),"\n",(0,t.jsx)(n.li,{children:"Document any failures or unexpected behaviors"}),"\n",(0,t.jsx)(n.li,{children:"Collect user feedback on system performance"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"1563-assessment-rubric",children:"15.6.3 Assessment Rubric"}),"\n",(0,t.jsx)(n.p,{children:"The capstone project will be assessed using the following rubric:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Criteria"}),(0,t.jsx)(n.th,{children:"Excellent (4)"}),(0,t.jsx)(n.th,{children:"Good (3)"}),(0,t.jsx)(n.th,{children:"Satisfactory (2)"}),(0,t.jsx)(n.th,{children:"Needs Improvement (1)"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"System Integration"})}),(0,t.jsx)(n.td,{children:"All components work seamlessly together with robust error handling"}),(0,t.jsx)(n.td,{children:"Most components integrated with minor issues"}),(0,t.jsx)(n.td,{children:"Basic integration achieved but with significant limitations"}),(0,t.jsx)(n.td,{children:"Components fail to integrate properly"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Task Performance"})}),(0,t.jsx)(n.td,{children:'Successfully completes "pick up the red cup" task with >90% success rate'}),(0,t.jsx)(n.td,{children:"80-90% success rate with good performance"}),(0,t.jsx)(n.td,{children:"60-80% success rate with acceptable performance"}),(0,t.jsx)(n.td,{children:"<60% success rate or task not completed"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Safety Implementation"})}),(0,t.jsx)(n.td,{children:"Comprehensive safety system with multiple fail-safes"}),(0,t.jsx)(n.td,{children:"Good safety implementation with some limitations"}),(0,t.jsx)(n.td,{children:"Basic safety measures implemented"}),(0,t.jsx)(n.td,{children:"Insufficient safety measures"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Documentation"})}),(0,t.jsx)(n.td,{children:"Comprehensive, clear, and well-organized documentation"}),(0,t.jsx)(n.td,{children:"Good documentation with minor gaps"}),(0,t.jsx)(n.td,{children:"Adequate documentation covering main components"}),(0,t.jsx)(n.td,{children:"Poor or incomplete documentation"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Presentation"})}),(0,t.jsx)(n.td,{children:"Clear, engaging presentation demonstrating deep understanding"}),(0,t.jsx)(n.td,{children:"Good presentation with solid understanding"}),(0,t.jsx)(n.td,{children:"Adequate presentation covering main points"}),(0,t.jsx)(n.td,{children:"Unclear or incomplete presentation"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"157-implementation-guide",children:"15.7 Implementation Guide"}),"\n",(0,t.jsx)(n.h3,{id:"1571-code-structure",children:"15.7.1 Code Structure"}),"\n",(0,t.jsx)(n.p,{children:"The complete system should follow this structure:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{children:"capstone_project/\n\u251c\u2500\u2500 src/\n\u2502   \u251c\u2500\u2500 voice_processing/\n\u2502   \u2502   \u251c\u2500\u2500 speech_recognition.py\n\u2502   \u2502   \u251c\u2500\u2500 nlu_processor.py\n\u2502   \u2502   \u2514\u2500\u2500 command_validator.py\n\u2502   \u251c\u2500\u2500 perception/\n\u2502   \u2502   \u251c\u2500\u2500 vslam_system.py\n\u2502   \u2502   \u251c\u2500\u2500 object_detection.py\n\u2502   \u2502   \u2514\u2500\u2500 fusion_system.py\n\u2502   \u251c\u2500\u2500 planning/\n\u2502   \u2502   \u251c\u2500\u2500 task_planner.py\n\u2502   \u2502   \u251c\u2500\u2500 motion_planner.py\n\u2502   \u2502   \u2514\u2500\u2500 safety_checker.py\n\u2502   \u251c\u2500\u2500 execution/\n\u2502   \u2502   \u251c\u2500\u2500 action_executor.py\n\u2502   \u2502   \u251c\u2500\u2500 monitoring_system.py\n\u2502   \u2502   \u2514\u2500\u2500 recovery_system.py\n\u2502   \u2514\u2500\u2500 main_system.py\n\u251c\u2500\u2500 config/\n\u2502   \u251c\u2500\u2500 system_params.yaml\n\u2502   \u251c\u2500\u2500 robot_description.urdf\n\u2502   \u2514\u2500\u2500 camera_calibration.yaml\n\u251c\u2500\u2500 launch/\n\u2502   \u2514\u2500\u2500 capstone_system_launch.py\n\u251c\u2500\u2500 test/\n\u2502   \u251c\u2500\u2500 unit_tests/\n\u2502   \u251c\u2500\u2500 integration_tests/\n\u2502   \u2514\u2500\u2500 evaluation_scripts/\n\u2514\u2500\u2500 docs/\n    \u251c\u2500\u2500 system_architecture.md\n    \u251c\u2500\u2500 user_manual.md\n    \u2514\u2500\u2500 evaluation_results.md\n"})}),"\n",(0,t.jsx)(n.h3,{id:"1572-main-system-integration",children:"15.7.2 Main System Integration"}),"\n",(0,t.jsx)(n.p,{children:"Here's the main system integration code:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.qos import QoSProfile\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import Image, AudioData\nfrom geometry_msgs.msg import PoseStamped\nfrom action_msgs.msg import GoalStatus\n\nfrom voice_processing import VoiceProcessingModule\nfrom perception import PerceptionModule\nfrom planning import PlanningModule\nfrom execution import ExecutionModule\n\nclass CapstoneSystem(Node):\n    def __init__(self):\n        super().__init__('capstone_system')\n\n        # Initialize all modules\n        self.voice_module = VoiceProcessingModule()\n        self.perception_module = PerceptionModule()\n        self.planning_module = PlanningModule()\n        self.execution_module = ExecutionModule()\n\n        # System state\n        self.system_ready = False\n        self.current_task = None\n        self.world_state = {}\n\n        # Publishers and subscribers\n        qos_profile = QoSProfile(depth=10)\n\n        self.voice_sub = self.create_subscription(\n            AudioData, 'audio_input', self.voice_callback, qos_profile\n        )\n\n        self.command_sub = self.create_subscription(\n            String, 'spoken_command', self.command_callback, qos_profile\n        )\n\n        self.status_pub = self.create_publisher(String, 'system_status', qos_profile)\n        self.ready_pub = self.create_publisher(Bool, 'system_ready', qos_profile)\n\n        # Initialize and check system status\n        self.initialize_system()\n\n    def initialize_system(self):\n        \"\"\"Initialize all system components\"\"\"\n        try:\n            # Initialize each module\n            self.voice_module.initialize()\n            self.perception_module.initialize()\n            self.planning_module.initialize()\n            self.execution_module.initialize()\n\n            # Check if all modules are ready\n            if (self.voice_module.is_ready() and\n                self.perception_module.is_ready() and\n                self.planning_module.is_ready() and\n                self.execution_module.is_ready()):\n\n                self.system_ready = True\n                self.get_logger().info('Capstone system initialized successfully')\n\n                # Publish ready status\n                ready_msg = Bool()\n                ready_msg.data = True\n                self.ready_pub.publish(ready_msg)\n            else:\n                self.get_logger().error('One or more modules failed to initialize')\n\n        except Exception as e:\n            self.get_logger().error(f'Error initializing system: {e}')\n            self.system_ready = False\n\n    def voice_callback(self, msg):\n        \"\"\"Handle incoming voice commands\"\"\"\n        if not self.system_ready:\n            self.get_logger().warn('System not ready to process voice commands')\n            return\n\n        try:\n            # Process voice command\n            command = self.voice_module.process_voice_command(msg.data)\n            self.get_logger().info(f'Processed command: {command}')\n\n            # Execute the command\n            success = self.execute_command(command)\n\n            # Publish status\n            status_msg = String()\n            status_msg.data = f'Command execution: {\"SUCCESS\" if success else \"FAILED\"}'\n            self.status_pub.publish(status_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing voice command: {e}')\n            status_msg = String()\n            status_msg.data = f'ERROR: {str(e)}'\n            self.status_pub.publish(status_msg)\n\n    def command_callback(self, msg):\n        \"\"\"Handle text commands (for testing)\"\"\"\n        if not self.system_ready:\n            self.get_logger().warn('System not ready to process commands')\n            return\n\n        try:\n            # Parse and execute command\n            command = self.voice_module.nlu_processor.parse_command(msg.data)\n            success = self.execute_command(command)\n\n            # Publish status\n            status_msg = String()\n            status_msg.data = f'Text command execution: {\"SUCCESS\" if success else \"FAILED\"}'\n            self.status_pub.publish(status_msg)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing text command: {e}')\n            status_msg = String()\n            status_msg.data = f'ERROR: {str(e)}'\n            self.status_pub.publish(status_msg)\n\n    def execute_command(self, command):\n        \"\"\"Execute a parsed command\"\"\"\n        try:\n            self.get_logger().info(f'Executing command: {command}')\n\n            # Update world state with current perception\n            world_objects, robot_pose = self.perception_module.perceive_environment()\n            self.world_state = {\n                'objects': world_objects,\n                'robot_pose': robot_pose,\n                'timestamp': self.get_clock().now()\n            }\n\n            # Plan the action sequence\n            plan = self.planning_module.plan_action_sequence(command, self.world_state)\n            self.get_logger().info(f'Generated plan with {len(plan)} steps')\n\n            # Execute the plan\n            success = self.execution_module.execute_plan(plan, self.world_state)\n\n            return success\n\n        except Exception as e:\n            self.get_logger().error(f'Error executing command: {e}')\n            return False\n\n    def shutdown(self):\n        \"\"\"Clean shutdown of the system\"\"\"\n        self.get_logger().info('Shutting down capstone system...')\n\n        # Stop all modules gracefully\n        self.voice_module.shutdown()\n        self.perception_module.shutdown()\n        self.planning_module.shutdown()\n        self.execution_module.shutdown()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    capstone_system = CapstoneSystem()\n\n    try:\n        rclpy.spin(capstone_system)\n    except KeyboardInterrupt:\n        capstone_system.get_logger().info('Interrupted by user')\n    finally:\n        capstone_system.shutdown()\n        capstone_system.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"1573-launch-file",children:"15.7.3 Launch File"}),"\n",(0,t.jsx)(n.p,{children:"Create a launch file to start all components:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"from launch import LaunchDescription\nfrom launch.actions import DeclareLaunchArgument, RegisterEventHandler\nfrom launch.event_handlers import OnProcessStart\nfrom launch.substitutions import LaunchConfiguration, PathJoinSubstitution\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\n\ndef generate_launch_description():\n    # Declare launch arguments\n    namespace = LaunchConfiguration('namespace')\n    use_sim_time = LaunchConfiguration('use_sim_time')\n    params_file = LaunchConfiguration('params_file')\n\n    # Voice processing node\n    voice_node = Node(\n        package='capstone_project',\n        executable='voice_processing_node',\n        name='voice_processing',\n        namespace=namespace,\n        parameters=[params_file],\n        output='screen'\n    )\n\n    # Perception node\n    perception_node = Node(\n        package='capstone_project',\n        executable='perception_node',\n        name='perception',\n        namespace=namespace,\n        parameters=[params_file],\n        remappings=[\n            ('/camera/rgb/image_raw', '/camera/rgb/image_raw'),\n            ('/camera/rgb/camera_info', '/camera/rgb/camera_info'),\n        ],\n        output='screen'\n    )\n\n    # Planning node\n    planning_node = Node(\n        package='capstone_project',\n        executable='planning_node',\n        name='planning',\n        namespace=namespace,\n        parameters=[params_file],\n        output='screen'\n    )\n\n    # Execution node\n    execution_node = Node(\n        package='capstone_project',\n        executable='execution_node',\n        name='execution',\n        namespace=namespace,\n        parameters=[params_file],\n        output='screen'\n    )\n\n    # Main capstone system node\n    capstone_node = Node(\n        package='capstone_project',\n        executable='capstone_system',\n        name='capstone_system',\n        namespace=namespace,\n        parameters=[params_file],\n        output='screen'\n    )\n\n    # RViz for visualization\n    rviz_config_file = PathJoinSubstitution([\n        FindPackageShare('capstone_project'),\n        'rviz',\n        'capstone_system.rviz'\n    ])\n\n    rviz_node = Node(\n        package='rviz2',\n        executable='rviz2',\n        name='rviz2',\n        arguments=['-d', rviz_config_file],\n        condition=LaunchConfiguration('enable_rviz')\n    )\n\n    # Create launch description\n    ld = LaunchDescription()\n\n    # Add launch arguments\n    ld.add_action(DeclareLaunchArgument(\n        'namespace',\n        default_value='',\n        description='Top-level namespace all nodes will be placed under'))\n\n    ld.add_action(DeclareLaunchArgument(\n        'use_sim_time',\n        default_value='false',\n        description='Use simulation (Gazebo) clock if true'))\n\n    ld.add_action(DeclareLaunchArgument(\n        'params_file',\n        default_value=PathJoinSubstitution([\n            FindPackageShare('capstone_project'),\n            'config',\n            'capstone_params.yaml'\n        ]),\n        description='Full path to the ROS2 parameters file to use for all launched nodes'))\n\n    ld.add_action(DeclareLaunchArgument(\n        'enable_rviz',\n        default_value='true',\n        description='Launch RViz2 for visualization'))\n\n    # Add nodes\n    ld.add_action(voice_node)\n    ld.add_action(perception_node)\n    ld.add_action(planning_node)\n    ld.add_action(execution_node)\n    ld.add_action(capstone_node)\n    ld.add_action(rviz_node)\n\n    return ld\n"})}),"\n",(0,t.jsx)(n.h2,{id:"158-troubleshooting-and-debugging",children:"15.8 Troubleshooting and Debugging"}),"\n",(0,t.jsx)(n.h3,{id:"1581-common-issues",children:"15.8.1 Common Issues"}),"\n",(0,t.jsx)(n.h4,{id:"integration-issues",children:"Integration Issues"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Message format mismatches"}),": Ensure all nodes use compatible message types"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Timing issues"}),": Use appropriate QoS profiles and consider message buffering"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Parameter configuration"}),": Verify all modules have correct parameter values"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"performance-issues",children:"Performance Issues"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Latency"}),": Optimize processing pipelines and consider parallel processing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Resource usage"}),": Monitor CPU, GPU, and memory usage"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time constraints"}),": Implement priority-based scheduling"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"safety-issues",children:"Safety Issues"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Emergency stop"}),": Ensure emergency stop functionality works in all states"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Safe fallbacks"}),": Implement safe states when components fail"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Monitoring"}),": Continuously monitor system health and performance"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"1582-debugging-tools",children:"15.8.2 Debugging Tools"}),"\n",(0,t.jsx)(n.h4,{id:"ros-2-tools",children:"ROS 2 Tools"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"ros2 topic echo"}),": Monitor message flow between nodes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"ros2 service list"}),": Check available services"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"rqt_graph"}),": Visualize node connections"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"ros2 bag"}),": Record and replay system data"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"custom-debugging",children:"Custom Debugging"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'class DebuggingTools:\n    def __init__(self, node):\n        self.node = node\n        self.debug_pub = node.create_publisher(String, \'debug_info\', 10)\n\n    def log_state(self, component, state_data):\n        """Log component state for debugging"""\n        debug_msg = String()\n        debug_msg.data = f\'{component}: {state_data}\'\n        self.debug_pub.publish(debug_msg)\n\n    def visualize_plan(self, plan, robot_pose):\n        """Visualize planned path in RViz"""\n        # Implementation would publish visualization markers\n        pass\n'})}),"\n",(0,t.jsx)(n.h2,{id:"159-conclusion",children:"15.9 Conclusion"}),"\n",(0,t.jsx)(n.p,{children:"The capstone project represents the ultimate test of the knowledge and skills developed throughout this textbook. Successfully implementing an autonomous humanoid system that responds to spoken commands requires integrating multiple complex technologies while maintaining safety and reliability. This project not only demonstrates technical competency but also showcases the ability to manage a complex, multi-disciplinary engineering challenge."}),"\n",(0,t.jsx)(n.p,{children:"Students completing this capstone will have gained invaluable experience in:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"System integration and architecture"}),"\n",(0,t.jsx)(n.li,{children:"Real-time processing and decision making"}),"\n",(0,t.jsx)(n.li,{children:"Safety-critical system design"}),"\n",(0,t.jsx)(n.li,{children:"Multi-modal AI systems"}),"\n",(0,t.jsx)(n.li,{children:"Human-robot interaction"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The skills developed through this project provide a strong foundation for careers in robotics, AI, and autonomous systems."}),"\n",(0,t.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Implementation Exercise"}),": Complete the full capstone system implementation following the architecture described in this chapter."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Evaluation Exercise"}),": Conduct a comprehensive evaluation of your system using the metrics and protocols described."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Extension Exercise"}),": Extend your system to handle more complex commands involving multiple objects or sequential actions."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Optimization Exercise"}),": Optimize your system for better performance, either in speed, accuracy, or resource usage."]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'OpenVLA Team (2024). "OpenVLA: An Open-Source Vision-Language-Action Model for Robotics."'}),"\n",(0,t.jsx)(n.li,{children:'NVIDIA Isaac Team (2024). "Isaac Sim and Isaac ROS Documentation."'}),"\n",(0,t.jsx)(n.li,{children:'ROS 2 Documentation. "Robot Operating System 2: Concepts and Best Practices."'}),"\n",(0,t.jsx)(n.li,{children:'Khatib, O., et al. (2018). "Humanoid Robots: From Concept to Reality."'}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>r});var t=s(6540);const i={},o=t.createContext(i);function l(e){const n=t.useContext(o);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);
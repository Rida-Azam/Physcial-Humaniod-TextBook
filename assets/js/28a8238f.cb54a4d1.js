"use strict";(self.webpackChunkphysical_ai_textbook=self.webpackChunkphysical_ai_textbook||[]).push([[217],{7866:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>t,default:()=>p,frontMatter:()=>s,metadata:()=>o,toc:()=>c});var i=a(4848),r=a(8453);const s={id:"chapter12",sidebar_position:3,title:"Dexterous Manipulation & Sim-to-Real Grasp Transfer"},t="Chapter 12: Dexterous Manipulation & Sim-to-Real Grasp Transfer",o={id:"module3/chapter12",title:"Dexterous Manipulation & Sim-to-Real Grasp Transfer",description:"Learning Objectives",source:"@site/docs/module3/chapter12.mdx",sourceDirName:"module3",slug:"/module3/chapter12",permalink:"/physical-ai-textbook/docs/module3/chapter12",draft:!1,unlisted:!1,editUrl:"https://github.com/your-github-username/physical-ai-textbook/tree/main/docs/module3/chapter12.mdx",tags:[],version:"current",sidebarPosition:3,frontMatter:{id:"chapter12",sidebar_position:3,title:"Dexterous Manipulation & Sim-to-Real Grasp Transfer"},sidebar:"tutorialSidebar",previous:{title:"Bipedal Locomotion \u2013 ZMP \u2192 MPC \u2192 RL Walking Policies",permalink:"/physical-ai-textbook/docs/module3/chapter11"},next:{title:"Vision-Language-Action Models",permalink:"/physical-ai-textbook/docs/module4/chapter13"}},l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"12.1 Introduction to Dexterous Manipulation",id:"121-introduction-to-dexterous-manipulation",level:2},{value:"12.2 Grasp Types and Classification",id:"122-grasp-types-and-classification",level:2},{value:"12.2.1 Power Grasps",id:"1221-power-grasps",level:3},{value:"12.2.2 Precision Grasps",id:"1222-precision-grasps",level:3},{value:"12.2.3 Grasp Stability Metrics",id:"1223-grasp-stability-metrics",level:3},{value:"12.3 Simulation Environments for Manipulation",id:"123-simulation-environments-for-manipulation",level:2},{value:"12.3.1 Physics Simulation Considerations",id:"1231-physics-simulation-considerations",level:3},{value:"12.3.2 Domain Randomization",id:"1232-domain-randomization",level:3},{value:"12.4 Sim-to-Real Transfer Techniques",id:"124-sim-to-real-transfer-techniques",level:2},{value:"12.4.1 System Identification",id:"1241-system-identification",level:3},{value:"12.4.2 Domain Adaptation",id:"1242-domain-adaptation",level:3},{value:"12.4.3 System Dynamics Randomization",id:"1243-system-dynamics-randomization",level:3},{value:"12.5 Tactile Sensing for Grasp Control",id:"125-tactile-sensing-for-grasp-control",level:2},{value:"12.5.1 Tactile Sensor Technologies",id:"1251-tactile-sensor-technologies",level:3},{value:"12.5.2 Tactile Feedback Integration",id:"1252-tactile-feedback-integration",level:3},{value:"12.6 Grasp Planning Algorithms",id:"126-grasp-planning-algorithms",level:2},{value:"12.6.1 Analytical Grasp Planning",id:"1261-analytical-grasp-planning",level:3},{value:"12.6.2 Learning-Based Grasp Planning",id:"1262-learning-based-grasp-planning",level:3},{value:"12.7 Implementation Example: Grasp Transfer Pipeline",id:"127-implementation-example-grasp-transfer-pipeline",level:2},{value:"12.8 Exercises",id:"128-exercises",level:2},{value:"12.9 Chapter Summary",id:"129-chapter-summary",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...n.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(e.h1,{id:"chapter-12-dexterous-manipulation--sim-to-real-grasp-transfer",children:"Chapter 12: Dexterous Manipulation & Sim-to-Real Grasp Transfer"}),"\n",(0,i.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsx)(e.li,{children:"Understand the principles of dexterous manipulation in humanoid robots"}),"\n",(0,i.jsx)(e.li,{children:"Analyze different grasp types and their applications"}),"\n",(0,i.jsx)(e.li,{children:"Implement sim-to-real transfer techniques for grasp learning"}),"\n",(0,i.jsx)(e.li,{children:"Design manipulation controllers for humanoid robots"}),"\n",(0,i.jsx)(e.li,{children:"Evaluate grasp success and robustness across simulation and real environments"}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"121-introduction-to-dexterous-manipulation",children:"12.1 Introduction to Dexterous Manipulation"}),"\n",(0,i.jsx)(e.p,{children:"Dexterous manipulation refers to the ability of robotic systems to perform complex manipulation tasks with high precision and adaptability. For humanoid robots, dexterous manipulation is essential for performing human-like tasks in unstructured environments."}),"\n",(0,i.jsx)(e.p,{children:"Key aspects of dexterous manipulation include:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Grasp Planning"}),": Determining optimal grasp points and configurations"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Force Control"}),": Managing contact forces during manipulation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Tactile Sensing"}),": Using touch feedback for fine manipulation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Multi-finger Coordination"}),": Coordinating multiple fingers for complex grasps"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"122-grasp-types-and-classification",children:"12.2 Grasp Types and Classification"}),"\n",(0,i.jsx)(e.h3,{id:"1221-power-grasps",children:"12.2.1 Power Grasps"}),"\n",(0,i.jsx)(e.p,{children:"Power grasps focus on stability and the ability to handle heavy objects. Common power grasps include:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Cylindrical Grasp"}),": Wrapping fingers around cylindrical objects"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Spherical Grasp"}),": Grasping spherical objects with finger pads"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Hook Grasp"}),": Using fingertips to grasp objects with handles"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"1222-precision-grasps",children:"12.2.2 Precision Grasps"}),"\n",(0,i.jsx)(e.p,{children:"Precision grasps prioritize fine control and dexterity over strength:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Pinch Grasp"}),": Using thumb and finger tips for precise positioning"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Lateral Grasp"}),": Grasping objects between thumb and index finger side"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Tripod Grasp"}),": Using thumb, index, and middle fingers for fine control"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"1223-grasp-stability-metrics",children:"12.2.3 Grasp Stability Metrics"}),"\n",(0,i.jsx)(e.p,{children:"Quantifying grasp quality is essential for grasp planning:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Example grasp stability evaluation\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\ndef evaluate_grasp_quality(contact_points, contact_forces, object_mass, gravity=9.81):\n    """\n    Evaluate grasp quality based on force closure and stability metrics\n    """\n    # Convert to numpy arrays\n    contacts = np.array(contact_points)\n    forces = np.array(contact_forces)\n\n    # Calculate center of mass of contact points\n    com = np.mean(contacts, axis=0)\n\n    # Calculate grasp matrix\n    # Each contact contributes to the grasp matrix with normal and friction cone\n    grasp_matrix = []\n    for i, (contact, force) in enumerate(zip(contacts, forces)):\n        # Normal vector (assuming force direction is normal)\n        n = force / np.linalg.norm(force)\n\n        # Tangential vectors (for friction cone)\n        t1 = np.cross(n, [1, 0, 0])\n        if np.linalg.norm(t1) < 1e-6:\n            t1 = np.cross(n, [0, 1, 0])\n        t1 = t1 / np.linalg.norm(t1)\n        t2 = np.cross(n, t1)\n        t2 = t2 / np.linalg.norm(t2)\n\n        # Grasp matrix row for this contact\n        # Force components: normal, tangential 1, tangential 2\n        # Moment components: cross product with contact point\n        row1 = np.hstack([n, np.cross(contact - com, n)])\n        row2 = np.hstack([t1, np.cross(contact - com, t1)])\n        row3 = np.hstack([t2, np.cross(contact - com, t2)])\n\n        grasp_matrix.extend([row1, row2, row3])\n\n    grasp_matrix = np.array(grasp_matrix)\n\n    # Calculate grasp quality metric (condition number of grasp matrix)\n    # Lower condition number indicates better force distribution\n    if grasp_matrix.shape[0] >= grasp_matrix.shape[1]:\n        U, s, Vt = np.linalg.svd(grasp_matrix)\n        condition_number = s[0] / s[-1] if s[-1] != 0 else float(\'inf\')\n        quality = 1.0 / condition_number if condition_number != float(\'inf\') else 0.0\n    else:\n        quality = 0.0\n\n    return quality\n\n# Example usage\ncontact_points = [\n    [0.1, 0.05, 0.0],\n    [0.1, -0.05, 0.0],\n    [-0.1, 0.0, 0.0]\n]\n\ncontact_forces = [\n    [0, 0, 5],  # Normal force\n    [0, 0, 5],  # Normal force\n    [0, 0, 10]  # Normal force\n]\n\nquality = evaluate_grasp_quality(contact_points, contact_forces, object_mass=0.5)\nprint(f"Grasp Quality: {quality}")\n'})}),"\n",(0,i.jsx)(e.h2,{id:"123-simulation-environments-for-manipulation",children:"12.3 Simulation Environments for Manipulation"}),"\n",(0,i.jsx)(e.h3,{id:"1231-physics-simulation-considerations",children:"12.3.1 Physics Simulation Considerations"}),"\n",(0,i.jsx)(e.p,{children:"Accurate physics simulation is crucial for sim-to-real transfer:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Friction Modeling"}),": Accurate representation of friction coefficients"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Contact Dynamics"}),": Realistic contact force computation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Material Properties"}),": Proper density, elasticity, and damping"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Sensor Simulation"}),": Realistic noise and latency models"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"1232-domain-randomization",children:"12.3.2 Domain Randomization"}),"\n",(0,i.jsx)(e.p,{children:"Domain randomization helps bridge the sim-to-real gap:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Example domain randomization for grasp training\nimport random\n\nclass DomainRandomization:\n    def __init__(self):\n        # Randomization ranges\n        self.friction_range = (0.3, 0.8)\n        self.mass_range = (0.1, 2.0)\n        self.com_offset_range = (-0.01, 0.01)\n        self.visual_noise_range = (0.0, 0.1)\n\n    def randomize_object_properties(self, object_params):\n        \"\"\"\n        Randomize object properties for domain randomization\n        \"\"\"\n        randomized_params = object_params.copy()\n\n        # Randomize friction\n        randomized_params['friction'] = random.uniform(*self.friction_range)\n\n        # Randomize mass\n        randomized_params['mass'] = random.uniform(*self.mass_range)\n\n        # Randomize center of mass offset\n        randomized_params['com_offset'] = [\n            random.uniform(*self.com_offset_range),\n            random.uniform(*self.com_offset_range),\n            random.uniform(*self.com_offset_range)\n        ]\n\n        return randomized_params\n\n    def randomize_visual_properties(self, visual_params):\n        \"\"\"\n        Randomize visual properties for domain randomization\n        \"\"\"\n        randomized_params = visual_params.copy()\n\n        # Add noise to visual features\n        noise_level = random.uniform(*self.visual_noise_range)\n        randomized_params['texture_noise'] = noise_level\n        randomized_params['lighting_variation'] = noise_level\n\n        return randomized_params\n\n# Example usage in simulation\ndomain_rand = DomainRandomization()\nobject_params = {\n    'friction': 0.5,\n    'mass': 0.5,\n    'com_offset': [0, 0, 0]\n}\n\nfor episode in range(1000):\n    # Randomize object properties each episode\n    randomized_obj = domain_rand.randomize_object_properties(object_params)\n    # Train grasp policy with randomized parameters\n"})}),"\n",(0,i.jsx)(e.h2,{id:"124-sim-to-real-transfer-techniques",children:"12.4 Sim-to-Real Transfer Techniques"}),"\n",(0,i.jsx)(e.h3,{id:"1241-system-identification",children:"12.4.1 System Identification"}),"\n",(0,i.jsx)(e.p,{children:"System identification helps characterize the differences between simulation and reality:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Parameter Estimation"}),": Identifying physical parameters that differ between sim and real"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Dynamics Modeling"}),": Learning the true dynamics from real-world data"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Calibration Procedures"}),": Adjusting simulation parameters to match real behavior"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"1242-domain-adaptation",children:"12.4.2 Domain Adaptation"}),"\n",(0,i.jsx)(e.p,{children:"Domain adaptation techniques help transfer learned policies:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:"# Example domain adaptation for grasp transfer\nimport torch\nimport torch.nn as nn\n\nclass GraspPolicy(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(GraspPolicy, self).__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, output_dim)\n        )\n\n    def forward(self, x):\n        return self.network(x)\n\nclass DomainDiscriminator(nn.Module):\n    def __init__(self, feature_dim):\n        super(DomainDiscriminator, self).__init__()\n        self.discriminator = nn.Sequential(\n            nn.Linear(feature_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        return self.discriminator(x)\n\nclass DomainAdaptationGrasp(nn.Module):\n    def __init__(self, input_dim, output_dim, feature_dim):\n        super(DomainAdaptationGrasp, self).__init__()\n        self.feature_extractor = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, feature_dim),\n            nn.ReLU()\n        )\n        self.policy = nn.Linear(feature_dim, output_dim)\n        self.discriminator = DomainDiscriminator(feature_dim)\n\n    def forward(self, x, domain='sim'):\n        features = self.feature_extractor(x)\n        action = self.policy(features)\n        domain_pred = self.discriminator(features)\n        return action, domain_pred\n\n# Training loop with domain adaptation\ndef train_domain_adaptation(sim_loader, real_loader, model, optimizer, epochs=100):\n    criterion = nn.MSELoss()\n    domain_criterion = nn.BCELoss()\n\n    for epoch in range(epochs):\n        for (sim_batch, real_batch) in zip(sim_loader, real_loader):\n            optimizer.zero_grad()\n\n            # Sim data (label as 0)\n            sim_actions, sim_domain = model(sim_batch, domain='sim')\n            sim_domain_loss = domain_criterion(sim_domain, torch.zeros_like(sim_domain))\n\n            # Real data (label as 1)\n            real_actions, real_domain = model(real_batch, domain='real')\n            real_domain_loss = domain_criterion(real_domain, torch.ones_like(real_domain))\n\n            # Domain adaptation loss (try to confuse discriminator)\n            total_domain_loss = sim_domain_loss + real_domain_loss\n            domain_adapt_loss = -total_domain_loss  # Minimize -loss to confuse discriminator\n\n            # Task loss (supervised on real data)\n            task_loss = criterion(real_actions, real_target_actions)  # Assuming we have targets\n\n            total_loss = task_loss + 0.1 * domain_adapt_loss\n            total_loss.backward()\n            optimizer.step()\n"})}),"\n",(0,i.jsx)(e.h3,{id:"1243-system-dynamics-randomization",children:"12.4.3 System Dynamics Randomization"}),"\n",(0,i.jsx)(e.p,{children:"Randomizing system dynamics during training improves robustness:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Actuator Dynamics"}),": Varying motor response characteristics"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Transmission Effects"}),": Modeling gear backlash and compliance"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Sensor Noise"}),": Adding realistic sensor noise and delays"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Environmental Conditions"}),": Varying surface properties and lighting"]}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"125-tactile-sensing-for-grasp-control",children:"12.5 Tactile Sensing for Grasp Control"}),"\n",(0,i.jsx)(e.h3,{id:"1251-tactile-sensor-technologies",children:"12.5.1 Tactile Sensor Technologies"}),"\n",(0,i.jsx)(e.p,{children:"Modern tactile sensors provide rich haptic feedback:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"GelSight Sensors"}),": High-resolution tactile imaging"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Optical Tactile Sensors"}),": Using cameras to detect surface deformation"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Piezoelectric Sensors"}),": Detecting force and vibration"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Capacitive Sensors"}),": Measuring proximity and contact area"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"1252-tactile-feedback-integration",children:"12.5.2 Tactile Feedback Integration"}),"\n",(0,i.jsx)(e.p,{children:"Integrating tactile feedback into grasp control:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Example tactile feedback controller\nimport numpy as np\n\nclass TactileGraspController:\n    def __init__(self, num_fingers=2):\n        self.num_fingers = num_fingers\n        self.tactile_threshold = 0.1  # Threshold for contact detection\n        self.force_limit = 50.0  # Maximum force before adjustment\n        self.slip_threshold = 0.05  # Threshold for slip detection\n\n    def process_tactile_data(self, tactile_readings):\n        """\n        Process tactile sensor data from multiple fingers\n        tactile_readings: list of [force_x, force_y, force_z, slip_detection] for each finger\n        """\n        contact_status = []\n        forces = []\n        slip_detected = []\n\n        for finger_data in tactile_readings:\n            force_mag = np.linalg.norm(finger_data[:3])\n            contact_status.append(force_mag > self.tactile_threshold)\n            forces.append(force_mag)\n            slip_detected.append(abs(finger_data[3]) > self.slip_threshold)\n\n        return contact_status, forces, slip_detected\n\n    def adjust_grasp(self, tactile_readings, current_forces):\n        """\n        Adjust grasp based on tactile feedback\n        """\n        contact_status, forces, slip_detected = self.process_tactile_data(tactile_readings)\n\n        new_forces = current_forces.copy()\n\n        for i, (contact, force, slip) in enumerate(zip(contact_status, forces, slip_detected)):\n            if slip:\n                # Increase force to prevent slip\n                new_forces[i] = min(new_forces[i] * 1.1, self.force_limit)\n            elif force > self.force_limit:\n                # Decrease force to avoid damage\n                new_forces[i] = max(new_forces[i] * 0.9, self.force_limit * 0.5)\n\n        return new_forces\n\n# Example usage\ntactile_controller = TactileGraspController(num_fingers=2)\n\n# Simulate tactile readings [force_x, force_y, force_z, slip_signal]\ntactile_readings = [\n    [2.0, 0.5, 15.0, 0.01],  # Finger 1\n    [1.8, 0.3, 14.5, 0.02]   # Finger 2\n]\n\ncurrent_forces = [20.0, 20.0]  # Current commanded forces\nnew_forces = tactile_controller.adjust_grasp(tactile_readings, current_forces)\nprint(f"Adjusted forces: {new_forces}")\n'})}),"\n",(0,i.jsx)(e.h2,{id:"126-grasp-planning-algorithms",children:"12.6 Grasp Planning Algorithms"}),"\n",(0,i.jsx)(e.h3,{id:"1261-analytical-grasp-planning",children:"12.6.1 Analytical Grasp Planning"}),"\n",(0,i.jsx)(e.p,{children:"Analytical methods compute grasp points based on object geometry:"}),"\n",(0,i.jsxs)(e.ul,{children:["\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Antipodal Grasps"}),": Finding pairs of contact points with opposing normals"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Geometric Features"}),": Using object edges, corners, and surfaces"]}),"\n",(0,i.jsxs)(e.li,{children:[(0,i.jsx)(e.strong,{children:"Force Closure Analysis"}),": Ensuring stable grasp configurations"]}),"\n"]}),"\n",(0,i.jsx)(e.h3,{id:"1262-learning-based-grasp-planning",children:"12.6.2 Learning-Based Grasp Planning"}),"\n",(0,i.jsx)(e.p,{children:"Learning approaches leverage data to improve grasp planning:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'# Example learning-based grasp planner\nimport numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\n\nclass LearningBasedGraspPlanner:\n    def __init__(self):\n        self.model = RandomForestRegressor(n_estimators=100)\n        self.is_trained = False\n\n    def extract_features(self, object_point_cloud, grasp_candidate):\n        """\n        Extract features for grasp evaluation\n        """\n        # Simplified feature extraction\n        # In practice, this would include geometric, shape, and spatial features\n        features = []\n\n        # Distance to object center\n        obj_center = np.mean(object_point_cloud, axis=0)\n        grasp_center = grasp_candidate[\'position\']\n        dist_to_center = np.linalg.norm(grasp_center - obj_center)\n        features.append(dist_to_center)\n\n        # Grasp width\n        features.append(grasp_candidate[\'width\'])\n\n        # Surface normal alignment\n        surface_normal = grasp_candidate[\'normal\']\n        approach_direction = grasp_candidate[\'approach\']\n        normal_alignment = abs(np.dot(surface_normal, approach_direction))\n        features.append(normal_alignment)\n\n        return np.array(features)\n\n    def train(self, training_data):\n        """\n        Train the grasp planner with training data\n        training_data: list of (object_point_cloud, grasp_candidate, success_probability)\n        """\n        X = []\n        y = []\n\n        for obj_pc, grasp, success_prob in training_data:\n            features = self.extract_features(obj_pc, grasp)\n            X.append(features)\n            y.append(success_prob)\n\n        X = np.array(X)\n        y = np.array(y)\n\n        self.model.fit(X, y)\n        self.is_trained = True\n\n    def predict_grasp_quality(self, object_point_cloud, grasp_candidate):\n        """\n        Predict the quality of a grasp candidate\n        """\n        if not self.is_trained:\n            raise ValueError("Model must be trained before prediction")\n\n        features = self.extract_features(object_point_cloud, grasp_candidate)\n        quality = self.model.predict([features])[0]\n\n        return max(0.0, min(1.0, quality))  # Clamp to [0, 1]\n\n    def plan_grasp(self, object_point_cloud, grasp_candidates):\n        """\n        Plan the best grasp from a set of candidates\n        """\n        best_grasp = None\n        best_quality = -1.0\n\n        for grasp in grasp_candidates:\n            quality = self.predict_grasp_quality(object_point_cloud, grasp)\n            if quality > best_quality:\n                best_quality = quality\n                best_grasp = grasp\n\n        return best_grasp, best_quality\n\n# Example usage (with simulated data)\ngrasp_planner = LearningBasedGraspPlanner()\n\n# Simulated training data (in practice, this would come from real grasping experiments)\ntraining_data = [\n    (np.random.random((100, 3)), {\'position\': [0.1, 0.0, 0.0], \'width\': 0.05, \'normal\': [0, 0, 1], \'approach\': [1, 0, 0]}, 0.8),\n    (np.random.random((100, 3)), {\'position\': [0.05, 0.05, 0.0], \'width\': 0.03, \'normal\': [0, 0, 1], \'approach\': [0, 1, 0]}, 0.6),\n    # ... more training examples\n]\n\n# In practice, you would have more training data\n# grasp_planner.train(training_data)\n'})}),"\n",(0,i.jsx)(e.h2,{id:"127-implementation-example-grasp-transfer-pipeline",children:"12.7 Implementation Example: Grasp Transfer Pipeline"}),"\n",(0,i.jsx)(e.p,{children:"A complete pipeline for sim-to-real grasp transfer:"}),"\n",(0,i.jsx)(e.pre,{children:(0,i.jsx)(e.code,{className:"language-python",children:'class GraspTransferPipeline:\n    def __init__(self):\n        self.simulation_env = None  # Initialized with simulation environment\n        self.domain_rand = DomainRandomization()\n        self.grasp_planner = LearningBasedGraspPlanner()\n        self.tactile_controller = TactileGraspController()\n        self.quality_evaluator = evaluate_grasp_quality\n\n    def train_in_simulation(self, num_episodes=10000):\n        """\n        Train grasp policy in simulation with domain randomization\n        """\n        for episode in range(num_episodes):\n            # Randomize environment parameters\n            randomized_params = self.domain_rand.randomize_object_properties({})\n\n            # Generate grasp candidates in simulation\n            object_pc = self.get_simulated_point_cloud()\n            grasp_candidates = self.generate_grasp_candidates(object_pc)\n\n            # Evaluate grasps in simulation\n            for grasp in grasp_candidates:\n                success = self.simulate_grasp(grasp, randomized_params)\n                # Store experience for learning\n\n    def transfer_to_real(self, real_object):\n        """\n        Transfer grasp policy to real robot\n        """\n        # Get point cloud of real object\n        object_pc = self.get_real_point_cloud(real_object)\n\n        # Plan grasp using trained policy\n        grasp_candidates = self.generate_grasp_candidates(object_pc)\n        best_grasp, quality = self.grasp_planner.plan_grasp(object_pc, grasp_candidates)\n\n        # Execute grasp with tactile feedback\n        execution_success = self.execute_grasp_with_feedback(best_grasp)\n\n        return execution_success, quality\n\n    def get_simulated_point_cloud(self):\n        """\n        Get point cloud from simulation\n        """\n        # Implementation depends on simulation environment\n        return np.random.random((500, 3))  # Placeholder\n\n    def get_real_point_cloud(self, object):\n        """\n        Get point cloud from real sensor\n        """\n        # Implementation depends on real sensor setup\n        return np.random.random((500, 3))  # Placeholder\n\n    def generate_grasp_candidates(self, point_cloud):\n        """\n        Generate potential grasp candidates\n        """\n        # Implementation of grasp candidate generation\n        candidates = []\n        for i in range(10):  # Generate 10 candidates\n            candidate = {\n                \'position\': point_cloud[i % len(point_cloud)],\n                \'width\': np.random.uniform(0.02, 0.08),\n                \'normal\': np.random.random(3),\n                \'approach\': np.random.random(3)\n            }\n            candidates.append(candidate)\n        return candidates\n\n    def simulate_grasp(self, grasp, params):\n        """\n        Simulate grasp execution\n        """\n        # Implementation depends on simulation environment\n        return np.random.random() > 0.5  # Placeholder success/failure\n\n    def execute_grasp_with_feedback(self, grasp):\n        """\n        Execute grasp on real robot with tactile feedback\n        """\n        # Implementation depends on real robot\n        return True  # Placeholder\n'})}),"\n",(0,i.jsx)(e.h2,{id:"128-exercises",children:"12.8 Exercises"}),"\n",(0,i.jsxs)(e.ol,{children:["\n",(0,i.jsx)(e.li,{children:"Implement a simple grasp planner for primitive objects (cubes, cylinders)."}),"\n",(0,i.jsx)(e.li,{children:"Train a grasp policy in simulation and test on real objects."}),"\n",(0,i.jsx)(e.li,{children:"Compare the performance of analytical vs. learning-based grasp planning."}),"\n",(0,i.jsx)(e.li,{children:"Design a tactile feedback controller for grasp stabilization."}),"\n"]}),"\n",(0,i.jsx)(e.h2,{id:"129-chapter-summary",children:"12.9 Chapter Summary"}),"\n",(0,i.jsx)(e.p,{children:"Dexterous manipulation in humanoid robots requires sophisticated grasp planning, force control, and tactile feedback integration. The sim-to-real transfer challenge can be addressed through domain randomization, system identification, and domain adaptation techniques. Successful manipulation requires combining analytical methods with learning-based approaches to achieve robust and adaptive grasping capabilities."})]})}function p(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,i.jsx)(e,{...n,children:(0,i.jsx)(d,{...n})}):d(n)}},8453:(n,e,a)=>{a.d(e,{R:()=>t,x:()=>o});var i=a(6540);const r={},s=i.createContext(r);function t(n){const e=i.useContext(s);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:t(n.components),i.createElement(s.Provider,{value:e},n.children)}}}]);
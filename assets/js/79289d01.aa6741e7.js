"use strict";(self.webpackChunkphysical_ai_textbook=self.webpackChunkphysical_ai_textbook||[]).push([[991],{7333:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>s,contentTitle:()=>l,default:()=>p,frontMatter:()=>r,metadata:()=>a,toc:()=>c});var i=o(4848),t=o(8453);const r={id:"chapter11",sidebar_position:2,title:"Bipedal Locomotion \u2013 ZMP \u2192 MPC \u2192 RL Walking Policies"},l="Chapter 11: Bipedal Locomotion \u2013 ZMP \u2192 MPC \u2192 RL Walking Policies",a={id:"module3/chapter11",title:"Bipedal Locomotion \u2013 ZMP \u2192 MPC \u2192 RL Walking Policies",description:"Learning Objectives",source:"@site/docs/module3/chapter11.mdx",sourceDirName:"module3",slug:"/module3/chapter11",permalink:"/physical-ai-textbook/docs/module3/chapter11",draft:!1,unlisted:!1,editUrl:"https://github.com/your-github-username/physical-ai-textbook/tree/main/docs/module3/chapter11.mdx",tags:[],version:"current",sidebarPosition:2,frontMatter:{id:"chapter11",sidebar_position:2,title:"Bipedal Locomotion \u2013 ZMP \u2192 MPC \u2192 RL Walking Policies"},sidebar:"tutorialSidebar",previous:{title:"Isaac ROS \u2013 Hardware-Accelerated Perception & Navigation",permalink:"/physical-ai-textbook/docs/module3/chapter10"},next:{title:"Dexterous Manipulation & Sim-to-Real Grasp Transfer",permalink:"/physical-ai-textbook/docs/module3/chapter12"}},s={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"11.1 Introduction to Bipedal Locomotion",id:"111-introduction-to-bipedal-locomotion",level:2},{value:"11.2 Zero Moment Point (ZMP) Control",id:"112-zero-moment-point-zmp-control",level:2},{value:"11.2.1 ZMP Fundamentals",id:"1121-zmp-fundamentals",level:3},{value:"11.2.2 ZMP-Based Walking Pattern Generation",id:"1122-zmp-based-walking-pattern-generation",level:3},{value:"11.2.3 Advantages and Limitations of ZMP Control",id:"1123-advantages-and-limitations-of-zmp-control",level:3},{value:"11.3 Model Predictive Control (MPC) for Locomotion",id:"113-model-predictive-control-mpc-for-locomotion",level:2},{value:"11.3.1 MPC Fundamentals",id:"1131-mpc-fundamentals",level:3},{value:"11.3.2 MPC for Bipedal Walking",id:"1132-mpc-for-bipedal-walking",level:3},{value:"11.4 Reinforcement Learning for Walking Policies",id:"114-reinforcement-learning-for-walking-policies",level:2},{value:"11.4.1 RL Framework for Locomotion",id:"1141-rl-framework-for-locomotion",level:3},{value:"11.4.2 Deep Reinforcement Learning Approaches",id:"1142-deep-reinforcement-learning-approaches",level:3},{value:"11.5 Comparison of Approaches",id:"115-comparison-of-approaches",level:2},{value:"11.6 Implementation Example: Hybrid Controller",id:"116-implementation-example-hybrid-controller",level:2},{value:"11.7 Exercises",id:"117-exercises",level:2},{value:"11.8 Chapter Summary",id:"118-chapter-summary",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h1,{id:"chapter-11-bipedal-locomotion--zmp--mpc--rl-walking-policies",children:"Chapter 11: Bipedal Locomotion \u2013 ZMP \u2192 MPC \u2192 RL Walking Policies"}),"\n",(0,i.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Understand the fundamentals of bipedal locomotion and stability"}),"\n",(0,i.jsx)(n.li,{children:"Compare Zero Moment Point (ZMP) control with Model Predictive Control (MPC)"}),"\n",(0,i.jsx)(n.li,{children:"Implement Reinforcement Learning (RL) approaches for walking policies"}),"\n",(0,i.jsx)(n.li,{children:"Design walking controllers for humanoid robots"}),"\n",(0,i.jsx)(n.li,{children:"Evaluate the trade-offs between different locomotion approaches"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"111-introduction-to-bipedal-locomotion",children:"11.1 Introduction to Bipedal Locomotion"}),"\n",(0,i.jsx)(n.p,{children:"Bipedal locomotion is one of the most challenging problems in robotics, requiring sophisticated control algorithms to maintain balance while achieving forward motion. Unlike wheeled robots, bipedal robots have limited points of contact with the ground, making them inherently unstable and requiring continuous balance control."}),"\n",(0,i.jsx)(n.p,{children:"The key challenges in bipedal locomotion include:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Maintaining balance during dynamic motion"}),"\n",(0,i.jsx)(n.li,{children:"Managing transitions between single and double support phases"}),"\n",(0,i.jsx)(n.li,{children:"Handling external disturbances and uneven terrain"}),"\n",(0,i.jsx)(n.li,{children:"Optimizing energy efficiency while maintaining stability"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"112-zero-moment-point-zmp-control",children:"11.2 Zero Moment Point (ZMP) Control"}),"\n",(0,i.jsx)(n.p,{children:"Zero Moment Point (ZMP) control is a classical approach to bipedal locomotion that focuses on maintaining the robot's center of pressure within the support polygon."}),"\n",(0,i.jsx)(n.h3,{id:"1121-zmp-fundamentals",children:"11.2.1 ZMP Fundamentals"}),"\n",(0,i.jsx)(n.p,{children:"The ZMP is defined as the point on the ground where the net moment of the ground reaction forces is zero. For stable locomotion, the ZMP must remain within the convex hull of the robot's feet (the support polygon)."}),"\n",(0,i.jsx)(n.p,{children:"Mathematically, the ZMP is calculated as:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"ZMP_x = (\u03a3(F_iz * x_i) - \u03a3(M_ix)) / \u03a3(F_iz)\nZMP_y = (\u03a3(F_iz * y_i) - \u03a3(M_iy)) / \u03a3(F_iz)\n"})}),"\n",(0,i.jsx)(n.p,{children:"Where F_iz are the vertical forces, (x_i, y_i) are the force application points, and M_ix, M_iy are the moments."}),"\n",(0,i.jsx)(n.h3,{id:"1122-zmp-based-walking-pattern-generation",children:"11.2.2 ZMP-Based Walking Pattern Generation"}),"\n",(0,i.jsx)(n.p,{children:"ZMP-based walking typically involves:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Preview Control"}),": Using future ZMP references to calculate optimal center of mass (CoM) trajectories"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Inverted Pendulum Models"}),": Simplifying the robot to a point mass supported by a variable-length leg"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Footstep Planning"}),": Pre-calculating footstep positions based on desired walking direction"]}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Example ZMP controller implementation\nimport numpy as np\nfrom scipy import signal\nimport math\n\nclass ZMPController:\n    def __init__(self, robot_mass, gravity=9.81, z_com=0.8):\n        self.mass = robot_mass\n        self.gravity = gravity\n        self.z_com = z_com  # Height of center of mass\n        self.omega = math.sqrt(gravity / z_com)\n\n        # Initialize state variables\n        self.com_x = 0.0\n        self.com_y = 0.0\n        self.com_z = z_com\n        self.com_dx = 0.0\n        self.com_dy = 0.0\n        self.com_dz = 0.0\n\n        # ZMP tracking\n        self.zmp_ref_x = 0.0\n        self.zmp_ref_y = 0.0\n\n    def update(self, dt, zmp_ref_x, zmp_ref_y):\n        """\n        Update ZMP controller with reference ZMP\n        """\n        # Update reference\n        self.zmp_ref_x = zmp_ref_x\n        self.zmp_ref_y = zmp_ref_y\n\n        # Calculate CoM trajectory based on ZMP reference\n        # Using the relationship: CoM_dd = omega^2 * (CoM - ZMP)\n        com_dd_x = self.omega**2 * (self.com_x - self.zmp_ref_x)\n        com_dd_y = self.omega**2 * (self.com_y - self.zmp_ref_y)\n\n        # Integrate to get velocity and position\n        self.com_dx += com_dd_x * dt\n        self.com_dy += com_dd_y * dt\n\n        self.com_x += self.com_dx * dt\n        self.com_y += self.com_dy * dt\n\n        # Return desired CoM position\n        return self.com_x, self.com_y, self.com_z\n\n# Example usage\nzmp_controller = ZMPController(robot_mass=50.0)\n'})}),"\n",(0,i.jsx)(n.h3,{id:"1123-advantages-and-limitations-of-zmp-control",children:"11.2.3 Advantages and Limitations of ZMP Control"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Advantages:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Well-established mathematical foundation"}),"\n",(0,i.jsx)(n.li,{children:"Proven stability properties"}),"\n",(0,i.jsx)(n.li,{children:"Computationally efficient for real-time control"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Limitations:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Assumes constant center of mass height"}),"\n",(0,i.jsx)(n.li,{children:"Limited adaptability to dynamic environments"}),"\n",(0,i.jsx)(n.li,{children:"Difficulty handling large disturbances"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"113-model-predictive-control-mpc-for-locomotion",children:"11.3 Model Predictive Control (MPC) for Locomotion"}),"\n",(0,i.jsx)(n.p,{children:"Model Predictive Control (MPC) offers a more flexible approach to bipedal locomotion by optimizing over a finite prediction horizon."}),"\n",(0,i.jsx)(n.h3,{id:"1131-mpc-fundamentals",children:"11.3.1 MPC Fundamentals"}),"\n",(0,i.jsx)(n.p,{children:"MPC solves an optimization problem at each time step:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"min \u03a3(k=0 to N-1) l(x_k, u_k) + l_N(x_N)\ns.t. x_{k+1} = f(x_k, u_k)\n     g(x_k, u_k) \u2264 0\n"})}),"\n",(0,i.jsx)(n.p,{children:"Where l is the stage cost, l_N is the terminal cost, and g represents constraints."}),"\n",(0,i.jsx)(n.h3,{id:"1132-mpc-for-bipedal-walking",children:"11.3.2 MPC for Bipedal Walking"}),"\n",(0,i.jsx)(n.p,{children:"In bipedal locomotion, MPC typically optimizes:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Center of mass trajectory"}),"\n",(0,i.jsx)(n.li,{children:"Footstep locations"}),"\n",(0,i.jsx)(n.li,{children:"Joint torques"}),"\n",(0,i.jsx)(n.li,{children:"Balance constraints"}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Example MPC-based walking controller\nimport numpy as np\nfrom scipy.optimize import minimize\n\nclass MPCLocomotionController:\n    def __init__(self, prediction_horizon=20, dt=0.1):\n        self.N = prediction_horizon  # Prediction horizon\n        self.dt = dt  # Time step\n        self.Q = np.eye(4)  # State cost matrix\n        self.R = np.eye(2)  # Control cost matrix\n        self.P = np.eye(4)  # Terminal cost matrix\n\n    def predict_dynamics(self, x, u):\n        """\n        Linearized dynamics model for CoM\n        x = [com_x, com_y, com_dx, com_dy]\n        u = [zmp_x, zmp_y]\n        """\n        A = np.array([\n            [1, 0, self.dt, 0],\n            [0, 1, 0, self.dt],\n            [self.dt, 0, 1, 0],\n            [0, self.dt, 0, 1]\n        ])\n\n        B = np.array([\n            [0, 0],\n            [0, 0],\n            [-self.dt, 0],\n            [0, -self.dt]\n        ])\n\n        return A @ x + B @ u\n\n    def cost_function(self, u_flat, x0, x_ref):\n        """\n        Cost function for MPC optimization\n        """\n        total_cost = 0.0\n        x = x0.copy()\n\n        # Reshape control sequence\n        U = u_flat.reshape((self.N, 2))\n\n        for k in range(self.N):\n            # Predict next state\n            x = self.predict_dynamics(x, U[k])\n\n            # State error cost\n            state_error = x - x_ref[k]\n            total_cost += state_error.T @ self.Q @ state_error\n\n            # Control effort cost\n            total_cost += U[k].T @ self.R @ U[k]\n\n        # Terminal cost\n        state_error = x - x_ref[self.N]\n        total_cost += state_error.T @ self.P @ state_error\n\n        return total_cost\n\n    def solve_mpc(self, x0, x_ref):\n        """\n        Solve MPC problem\n        """\n        # Initial guess for control sequence\n        u_init = np.zeros(2 * self.N)\n\n        # Optimization bounds (ZMP limits)\n        bounds = [(-0.1, 0.1), (-0.1, 0.1)] * self.N  # Example ZMP limits\n\n        # Solve optimization problem\n        result = minimize(\n            self.cost_function,\n            u_init,\n            args=(x0, x_ref),\n            method=\'SLSQP\',\n            bounds=bounds\n        )\n\n        if result.success:\n            U_opt = result.x.reshape((self.N, 2))\n            return U_opt[0]  # Return first control input\n        else:\n            # Return zero control if optimization fails\n            return np.array([0.0, 0.0])\n\n# Example usage\nmpc_controller = MPCLocomotionController()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"114-reinforcement-learning-for-walking-policies",children:"11.4 Reinforcement Learning for Walking Policies"}),"\n",(0,i.jsx)(n.p,{children:"Reinforcement Learning (RL) offers a data-driven approach to bipedal locomotion that can learn complex walking behaviors through interaction with the environment."}),"\n",(0,i.jsx)(n.h3,{id:"1141-rl-framework-for-locomotion",children:"11.4.1 RL Framework for Locomotion"}),"\n",(0,i.jsx)(n.p,{children:"The RL framework for bipedal locomotion typically involves:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"State Space"}),": Joint positions, velocities, IMU readings, contact states"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Action Space"}),": Joint torques, desired joint positions/velocities"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reward Function"}),": Balance maintenance, forward progress, energy efficiency"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Environment"}),": Simulation or real robot"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"1142-deep-reinforcement-learning-approaches",children:"11.4.2 Deep Reinforcement Learning Approaches"}),"\n",(0,i.jsx)(n.p,{children:"Deep RL algorithms commonly used for bipedal locomotion include:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Proximal Policy Optimization (PPO)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Soft Actor-Critic (SAC)"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.strong,{children:"Deep Deterministic Policy Gradient (DDPG)"})}),"\n"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# Example RL environment for bipedal walking\nimport gym\nfrom gym import spaces\nimport numpy as np\n\nclass BipedalWalkingEnv(gym.Env):\n    def __init__(self):\n        super(BipedalWalkingEnv, self).__init__()\n\n        # Define action and observation spaces\n        self.action_space = spaces.Box(\n            low=-1.0, high=1.0, shape=(12,), dtype=np.float32  # 12 joint torques\n        )\n\n        # Observation: joint positions, velocities, IMU readings, etc.\n        self.observation_space = spaces.Box(\n            low=-np.inf, high=np.inf, shape=(24,), dtype=np.float32\n        )\n\n        # Robot parameters\n        self.max_steps = 1000\n        self.current_step = 0\n\n    def reset(self):\n        # Reset robot to initial configuration\n        self.current_step = 0\n        observation = self._get_observation()\n        return observation\n\n    def step(self, action):\n        # Apply action to robot simulation\n        self._apply_action(action)\n\n        # Update simulation\n        self._update_simulation()\n\n        # Calculate reward\n        reward = self._calculate_reward()\n\n        # Check termination conditions\n        terminated = self._check_termination()\n        truncated = self.current_step >= self.max_steps\n\n        observation = self._get_observation()\n        info = {}\n\n        self.current_step += 1\n        return observation, reward, terminated, truncated, info\n\n    def _get_observation(self):\n        # Return current robot state (simplified)\n        obs = np.random.random(24).astype(np.float32)  # Placeholder\n        return obs\n\n    def _apply_action(self, action):\n        # Apply joint torques to robot (simulation)\n        pass\n\n    def _update_simulation(self):\n        # Update physics simulation\n        pass\n\n    def _calculate_reward(self):\n        # Calculate reward based on forward progress, balance, etc.\n        reward = 0.0\n\n        # Forward progress reward\n        # Balance maintenance reward\n        # Energy efficiency penalty\n        # Survival bonus\n\n        return reward\n\n    def _check_termination(self):\n        # Check if robot has fallen or reached goal\n        return False\n\n# Example training loop (pseudocode)\ndef train_walking_policy():\n    env = BipedalWalkingEnv()\n\n    # Initialize RL agent (e.g., PPO)\n    # agent = PPO(env)\n\n    # Training loop\n    for episode in range(1000):\n        obs = env.reset()\n        total_reward = 0\n\n        while True:\n            action = agent.get_action(obs)  # Get action from policy\n            next_obs, reward, terminated, truncated, info = env.step(action)\n            agent.update(obs, action, reward, next_obs, terminated)  # Update policy\n\n            obs = next_obs\n            total_reward += reward\n\n            if terminated or truncated:\n                break\n\n        print(f"Episode {episode}, Total Reward: {total_reward}")\n'})}),"\n",(0,i.jsx)(n.h2,{id:"115-comparison-of-approaches",children:"11.5 Comparison of Approaches"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Approach"}),(0,i.jsx)(n.th,{children:"Advantages"}),(0,i.jsx)(n.th,{children:"Disadvantages"}),(0,i.jsx)(n.th,{children:"Use Case"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"ZMP Control"}),(0,i.jsx)(n.td,{children:"Proven stability, computationally efficient"}),(0,i.jsx)(n.td,{children:"Limited adaptability, rigid assumptions"}),(0,i.jsx)(n.td,{children:"Precise, predictable walking on flat terrain"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"MPC"}),(0,i.jsx)(n.td,{children:"Flexible, handles constraints well"}),(0,i.jsx)(n.td,{children:"Computationally intensive"}),(0,i.jsx)(n.td,{children:"Complex terrain, obstacle avoidance"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"RL"}),(0,i.jsx)(n.td,{children:"Learns complex behaviors, adaptable"}),(0,i.jsx)(n.td,{children:"Requires extensive training, may be unstable"}),(0,i.jsx)(n.td,{children:"Dynamic environments, complex behaviors"})]})]})]}),"\n",(0,i.jsx)(n.h2,{id:"116-implementation-example-hybrid-controller",children:"11.6 Implementation Example: Hybrid Controller"}),"\n",(0,i.jsx)(n.p,{children:"A practical approach often combines multiple methods:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class HybridWalkingController:\n    def __init__(self):\n        # ZMP controller for basic stability\n        self.zmp_controller = ZMPController(robot_mass=50.0)\n\n        # MPC for trajectory optimization\n        self.mpc_controller = MPCLocomotionController()\n\n        # RL policy for adaptive behaviors\n        self.rl_policy = None  # Loaded from trained model\n\n    def compute_walking_command(self, robot_state, desired_velocity):\n        # Use ZMP for basic balance maintenance\n        zmp_ref = self._compute_zmp_reference(desired_velocity)\n\n        # Use MPC for optimal trajectory planning\n        mpc_command = self.mpc_controller.solve_mpc(\n            robot_state, zmp_ref\n        )\n\n        # Use RL for adaptive responses to disturbances\n        if self.rl_policy:\n            rl_command = self.rl_policy.get_action(robot_state)\n            # Blend MPC and RL commands\n            final_command = 0.7 * mpc_command + 0.3 * rl_command\n        else:\n            final_command = mpc_command\n\n        return final_command\n\n    def _compute_zmp_reference(self, desired_velocity):\n        # Compute ZMP reference based on desired velocity\n        # This is a simplified example\n        zmp_x_ref = desired_velocity[0] * 0.1  # Proportional to forward velocity\n        zmp_y_ref = desired_velocity[1] * 0.1  # Proportional to lateral velocity\n        return np.array([zmp_x_ref, zmp_y_ref])\n"})}),"\n",(0,i.jsx)(n.h2,{id:"117-exercises",children:"11.7 Exercises"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Implement a simple ZMP controller for a 2D bipedal model."}),"\n",(0,i.jsx)(n.li,{children:"Compare the stability of ZMP and MPC controllers in simulation."}),"\n",(0,i.jsx)(n.li,{children:"Train a basic RL policy for bipedal walking using a physics simulator."}),"\n",(0,i.jsx)(n.li,{children:"Design a hybrid controller that combines ZMP and RL approaches."}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"118-chapter-summary",children:"11.8 Chapter Summary"}),"\n",(0,i.jsx)(n.p,{children:"Bipedal locomotion requires sophisticated control strategies to maintain balance while achieving desired motion. ZMP control provides a mathematically sound foundation for stable walking, MPC offers flexibility for complex tasks, and RL enables adaptive behaviors through learning. A hybrid approach often provides the best balance of stability, adaptability, and performance for real-world humanoid robots."})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>l,x:()=>a});var i=o(6540);const t={},r=i.createContext(t);function l(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);
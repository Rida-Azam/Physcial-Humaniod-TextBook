"use strict";(self.webpackChunkphysical_ai_textbook=self.webpackChunkphysical_ai_textbook||[]).push([[555],{6339:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>p,frontMatter:()=>a,metadata:()=>r,toc:()=>c});var o=t(4848),i=t(8453);const a={id:"chapter14",sidebar_position:2,title:"From Voice \u2192 Plan \u2192 Action"},s="Chapter 14: From Voice \u2192 Plan \u2192 Action",r={id:"module4/chapter14",title:"From Voice \u2192 Plan \u2192 Action",description:"Learning Objectives",source:"@site/docs/module4/chapter14.mdx",sourceDirName:"module4",slug:"/module4/chapter14",permalink:"/physical-ai-textbook/docs/module4/chapter14",draft:!1,unlisted:!1,editUrl:"https://github.com/your-github-username/physical-ai-textbook/tree/main/docs/module4/chapter14.mdx",tags:[],version:"current",sidebarPosition:2,frontMatter:{id:"chapter14",sidebar_position:2,title:"From Voice \u2192 Plan \u2192 Action"},sidebar:"tutorialSidebar",previous:{title:"Vision-Language-Action Models",permalink:"/physical-ai-textbook/docs/module4/chapter13"},next:{title:"Capstone Project \u2013 Autonomous Humanoid from Spoken Command",permalink:"/physical-ai-textbook/docs/module4/chapter15"}},l={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"14.1 Introduction to Voice-Controlled Robotics",id:"141-introduction-to-voice-controlled-robotics",level:2},{value:"14.1.1 The Voice \u2192 Plan \u2192 Action Pipeline",id:"1411-the-voice--plan--action-pipeline",level:3},{value:"14.1.2 Challenges in Voice-Controlled Robotics",id:"1412-challenges-in-voice-controlled-robotics",level:3},{value:"14.2 Voice Processing Pipeline",id:"142-voice-processing-pipeline",level:2},{value:"14.2.1 Speech Recognition",id:"1421-speech-recognition",level:3},{value:"Whisper Architecture",id:"whisper-architecture",level:4},{value:"14.2.2 Real-time Audio Processing",id:"1422-real-time-audio-processing",level:3},{value:"14.3 Natural Language Understanding",id:"143-natural-language-understanding",level:2},{value:"14.3.1 Intent Recognition",id:"1431-intent-recognition",level:3},{value:"14.3.2 Semantic Parsing",id:"1432-semantic-parsing",level:3},{value:"14.4 Task Planning",id:"144-task-planning",level:2},{value:"14.4.1 Hierarchical Task Networks (HTN)",id:"1441-hierarchical-task-networks-htn",level:3},{value:"14.4.2 Integration with ROS 2",id:"1442-integration-with-ros-2",level:3},{value:"14.5 Voice Command Executor",id:"145-voice-command-executor",level:2},{value:"14.5.1 ROS 2 Action Server",id:"1451-ros-2-action-server",level:3},{value:"14.6 Safety and Error Handling",id:"146-safety-and-error-handling",level:2},{value:"14.6.1 Command Validation",id:"1461-command-validation",level:3},{value:"14.6.2 Error Recovery",id:"1462-error-recovery",level:3},{value:"14.7 Performance Optimization",id:"147-performance-optimization",level:2},{value:"14.7.1 Real-time Processing",id:"1471-real-time-processing",level:3},{value:"14.8 Summary",id:"148-summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"References",id:"references",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.h1,{id:"chapter-14-from-voice--plan--action",children:"Chapter 14: From Voice \u2192 Plan \u2192 Action"}),"\n",(0,o.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,o.jsx)(n.p,{children:"By the end of this chapter, students will be able to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:"Implement a voice processing pipeline using speech recognition models"}),"\n",(0,o.jsx)(n.li,{children:"Design a planning system that translates natural language commands into robot actions"}),"\n",(0,o.jsx)(n.li,{children:"Integrate voice processing, planning, and execution in a unified system"}),"\n",(0,o.jsx)(n.li,{children:"Create a robust ROS 2-based voice-controlled robot system"}),"\n",(0,o.jsx)(n.li,{children:"Evaluate the performance and safety of voice-controlled robotic systems"}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"141-introduction-to-voice-controlled-robotics",children:"14.1 Introduction to Voice-Controlled Robotics"}),"\n",(0,o.jsx)(n.p,{children:"Voice control represents one of the most intuitive interfaces for human-robot interaction. By enabling robots to understand and execute natural language commands, we bridge the gap between human intention and robotic action. This chapter explores the complete pipeline from voice input to robot execution, focusing on the challenges and solutions in each component."}),"\n",(0,o.jsx)(n.h3,{id:"1411-the-voice--plan--action-pipeline",children:"14.1.1 The Voice \u2192 Plan \u2192 Action Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"The voice-controlled robotic system follows a multi-stage pipeline:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{children:"Voice Input \u2192 Speech Recognition \u2192 Natural Language Understanding \u2192 Task Planning \u2192 Action Execution \u2192 Robot Control\n"})}),"\n",(0,o.jsx)(n.p,{children:"Each stage transforms the input from one representation to another, ultimately resulting in physical robot actions."}),"\n",(0,o.jsx)(n.h3,{id:"1412-challenges-in-voice-controlled-robotics",children:"14.1.2 Challenges in Voice-Controlled Robotics"}),"\n",(0,o.jsx)(n.p,{children:"Voice-controlled robotics faces several unique challenges:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Ambiguity in Natural Language"}),': Commands like "move the object" require contextual understanding']}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Real-time Processing Requirements"}),": The system must respond quickly to maintain natural interaction"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Noise and Environmental Factors"}),": Background noise can affect speech recognition accuracy"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robustness to Varying Commands"}),": The system should handle synonymous commands and unexpected phrasings"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"142-voice-processing-pipeline",children:"14.2 Voice Processing Pipeline"}),"\n",(0,o.jsx)(n.h3,{id:"1421-speech-recognition",children:"14.2.1 Speech Recognition"}),"\n",(0,o.jsx)(n.p,{children:"The first step in voice processing is converting audio to text. Modern systems use deep learning models like Whisper for this task."}),"\n",(0,o.jsx)(n.h4,{id:"whisper-architecture",children:"Whisper Architecture"}),"\n",(0,o.jsx)(n.p,{children:"Whisper is a transformer-based model that performs robust speech recognition across multiple languages. It's particularly effective because it's trained on a large and diverse dataset of audio-text pairs."}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import torch\nimport whisper\n\n# Load pre-trained Whisper model\nmodel = whisper.load_model("base")  # Options: tiny, base, small, medium, large\n\ndef transcribe_audio(audio_path):\n    """Transcribe audio to text using Whisper"""\n    result = model.transcribe(audio_path)\n    return result["text"]\n'})}),"\n",(0,o.jsx)(n.h3,{id:"1422-real-time-audio-processing",children:"14.2.2 Real-time Audio Processing"}),"\n",(0,o.jsx)(n.p,{children:"For real-time voice control, we need to process audio streams continuously:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import pyaudio\nimport numpy as np\nimport threading\nimport queue\n\nclass AudioProcessor:\n    def __init__(self, callback_func):\n        self.callback_func = callback_func\n        self.audio_queue = queue.Queue()\n        self.is_recording = False\n\n        # Audio parameters\n        self.chunk_size = 1024\n        self.format = pyaudio.paInt16\n        self.channels = 1\n        self.rate = 16000  # Whisper works well with 16kHz\n\n        self.audio = pyaudio.PyAudio()\n\n    def start_recording(self):\n        """Start audio recording in a separate thread"""\n        self.is_recording = True\n        recording_thread = threading.Thread(target=self._record_audio)\n        recording_thread.start()\n\n    def _record_audio(self):\n        """Internal method to record audio"""\n        stream = self.audio.open(\n            format=self.format,\n            channels=self.channels,\n            rate=self.rate,\n            input=True,\n            frames_per_buffer=self.chunk_size\n        )\n\n        while self.is_recording:\n            data = stream.read(self.chunk_size)\n            self.audio_queue.put(data)\n\n        stream.stop_stream()\n        stream.close()\n\n    def stop_recording(self):\n        """Stop audio recording"""\n        self.is_recording = False\n        self.audio.terminate()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"143-natural-language-understanding",children:"14.3 Natural Language Understanding"}),"\n",(0,o.jsx)(n.h3,{id:"1431-intent-recognition",children:"14.3.1 Intent Recognition"}),"\n",(0,o.jsx)(n.p,{children:"After converting speech to text, we need to understand the user's intent. This involves:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Intent Classification"}),": Determining the type of action requested"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Entity Extraction"}),": Identifying objects, locations, and other parameters"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Context Resolution"}),": Understanding references based on the current situation"]}),"\n"]}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import spacy\nfrom transformers import pipeline\n\nclass NaturalLanguageProcessor:\n    def __init__(self):\n        # Load spaCy model for linguistic processing\n        self.nlp = spacy.load(\"en_core_web_sm\")\n\n        # Initialize intent classification pipeline\n        self.intent_classifier = pipeline(\n            \"text-classification\",\n            model=\"microsoft/DialoGPT-medium\"  # Example model, replace with appropriate one\n        )\n\n    def process_command(self, text):\n        \"\"\"Process natural language command\"\"\"\n        doc = self.nlp(text)\n\n        # Extract intents and entities\n        intent = self.classify_intent(text)\n        entities = self.extract_entities(doc)\n        context = self.resolve_context(doc, entities)\n\n        return {\n            'intent': intent,\n            'entities': entities,\n            'context': context,\n            'original_text': text\n        }\n\n    def classify_intent(self, text):\n        \"\"\"Classify the intent of the command\"\"\"\n        # Example intents for robotic commands\n        if any(word in text.lower() for word in ['move', 'go', 'navigate', 'walk']):\n            return 'navigation'\n        elif any(word in text.lower() for word in ['pick', 'grasp', 'take', 'grab']):\n            return 'manipulation'\n        elif any(word in text.lower() for word in ['stop', 'halt', 'pause']):\n            return 'stop'\n        else:\n            return 'unknown'\n\n    def extract_entities(self, doc):\n        \"\"\"Extract named entities from the command\"\"\"\n        entities = {\n            'objects': [],\n            'locations': [],\n            'colors': [],\n            'sizes': []\n        }\n\n        for ent in doc.ents:\n            if ent.label_ in ['OBJECT', 'PRODUCT']:  # Custom NER model would label these\n                entities['objects'].append(ent.text)\n            elif ent.label_ in ['GPE', 'LOC']:  # Geographic places, locations\n                entities['locations'].append(ent.text)\n\n        # Extract colors and sizes based on adjectives\n        for token in doc:\n            if token.pos_ == 'ADJ':  # Adjective\n                if token.lemma_ in ['red', 'blue', 'green', 'yellow', 'black', 'white']:\n                    entities['colors'].append(token.text)\n                elif token.lemma_ in ['big', 'small', 'large', 'tiny', 'huge']:\n                    entities['sizes'].append(token.text)\n\n        return entities\n"})}),"\n",(0,o.jsx)(n.h3,{id:"1432-semantic-parsing",children:"14.3.2 Semantic Parsing"}),"\n",(0,o.jsx)(n.p,{children:"For more complex commands, we need to parse the semantic structure:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class SemanticParser:\n    def __init__(self):\n        self.action_templates = {\n            'navigation': ['go to {location}', 'move to {location}', 'navigate to {location}'],\n            'manipulation': ['pick up {object}', 'grasp {object}', 'take {object}'],\n            'action_object': ['move {object} to {location}']\n        }\n\n    def parse_command(self, command, entities):\n        \"\"\"Parse command into structured action\"\"\"\n        # Determine action type based on command structure\n        if 'location' in entities and any(obj for obj in entities['objects']):\n            # This is likely a move object action\n            return {\n                'action_type': 'move_object',\n                'object': entities['objects'][0] if entities['objects'] else None,\n                'destination': entities['locations'][0] if entities['locations'] else None\n            }\n        elif 'location' in entities:\n            # Navigation action\n            return {\n                'action_type': 'navigate',\n                'destination': entities['locations'][0] if entities['locations'] else None\n            }\n        elif 'object' in entities:\n            # Manipulation action\n            return {\n                'action_type': 'manipulate',\n                'object': entities['objects'][0] if entities['objects'] else None\n            }\n        else:\n            # Unknown action\n            return {\n                'action_type': 'unknown',\n                'command': command\n            }\n"})}),"\n",(0,o.jsx)(n.h2,{id:"144-task-planning",children:"14.4 Task Planning"}),"\n",(0,o.jsx)(n.h3,{id:"1441-hierarchical-task-networks-htn",children:"14.4.1 Hierarchical Task Networks (HTN)"}),"\n",(0,o.jsx)(n.p,{children:"For complex commands, we need a planning system that can decompose high-level goals into executable actions:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class TaskPlanner:\n    def __init__(self):\n        self.primitive_actions = {\n            'move_to': self._move_to,\n            'grasp_object': self._grasp_object,\n            'release_object': self._release_object,\n            'look_at': self._look_at\n        }\n\n        self.composite_tasks = {\n            'pick_and_place': self._pick_and_place,\n            'navigate_and_report': self._navigate_and_report\n        }\n\n    def plan_task(self, action_request):\n        \"\"\"Plan a sequence of actions for the given request\"\"\"\n        if action_request['action_type'] in self.composite_tasks:\n            return self.composite_tasks[action_request['action_type']](action_request)\n        else:\n            # Return primitive action\n            return [action_request]\n\n    def _pick_and_place(self, request):\n        \"\"\"Plan a pick and place task\"\"\"\n        object_to_pick = request.get('object')\n        destination = request.get('destination')\n\n        # Sequence of actions for pick and place\n        plan = [\n            {\n                'action': 'look_at',\n                'target': object_to_pick,\n                'description': f'Look at {object_to_pick}'\n            },\n            {\n                'action': 'move_to',\n                'target': f'near_{object_to_pick}',\n                'description': f'Move near {object_to_pick}'\n            },\n            {\n                'action': 'grasp_object',\n                'target': object_to_pick,\n                'description': f'Grasp {object_to_pick}'\n            },\n            {\n                'action': 'move_to',\n                'target': destination,\n                'description': f'Move to {destination}'\n            },\n            {\n                'action': 'release_object',\n                'target': object_to_pick,\n                'description': f'Release {object_to_pick}'\n            }\n        ]\n\n        return plan\n\n    def _navigate_and_report(self, request):\n        \"\"\"Plan navigation with reporting\"\"\"\n        destination = request.get('destination')\n\n        plan = [\n            {\n                'action': 'move_to',\n                'target': destination,\n                'description': f'Navigate to {destination}'\n            },\n            {\n                'action': 'report_arrival',\n                'target': destination,\n                'description': f'Report arrival at {destination}'\n            }\n        ]\n\n        return plan\n"})}),"\n",(0,o.jsx)(n.h3,{id:"1442-integration-with-ros-2",children:"14.4.2 Integration with ROS 2"}),"\n",(0,o.jsx)(n.p,{children:"The planner needs to interface with ROS 2 for action execution:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nfrom action_msgs.msg import GoalStatus\nfrom rclpy.action import ActionClient\n\nclass PlanningNode(Node):\n    def __init__(self):\n        super().__init__('planning_node')\n\n        # Initialize planner\n        self.task_planner = TaskPlanner()\n        self.nlp_processor = NaturalLanguageProcessor()\n        self.semantic_parser = SemanticParser()\n\n        # Publishers and subscribers\n        self.command_sub = self.create_subscription(\n            String,\n            'voice_command',\n            self.command_callback,\n            10\n        )\n\n        self.status_pub = self.create_publisher(\n            String,\n            'planning_status',\n            10\n        )\n\n        # Action clients for robot execution\n        self.nav_client = ActionClient(self, NavigateToPose, 'navigate_to_pose')\n        self.manip_client = ActionClient(self, GraspObject, 'grasp_object')\n\n        # Current plan and execution state\n        self.current_plan = []\n        self.current_step = 0\n        self.is_executing = False\n\n    def command_callback(self, msg):\n        \"\"\"Process incoming voice command\"\"\"\n        command_text = msg.data\n        self.get_logger().info(f'Received command: {command_text}')\n\n        # Process the command through the pipeline\n        nlp_result = self.nlp_processor.process_command(command_text)\n        semantic_action = self.semantic_parser.parse_command(command_text, nlp_result['entities'])\n        plan = self.task_planner.plan_task(semantic_action)\n\n        # Execute the plan\n        self.execute_plan(plan)\n\n    def execute_plan(self, plan):\n        \"\"\"Execute the planned sequence of actions\"\"\"\n        self.current_plan = plan\n        self.current_step = 0\n        self.is_executing = True\n\n        self.get_logger().info(f'Executing plan with {len(plan)} steps')\n\n        # Execute first step\n        if self.current_plan:\n            self.execute_current_step()\n\n    def execute_current_step(self):\n        \"\"\"Execute the current step in the plan\"\"\"\n        if self.current_step >= len(self.current_plan):\n            # Plan completed\n            self.plan_completed()\n            return\n\n        step = self.current_plan[self.current_step]\n        action_type = step['action']\n\n        self.get_logger().info(f'Executing step {self.current_step + 1}: {step[\"description\"]}')\n\n        # Execute the action based on type\n        if action_type == 'move_to':\n            self.execute_navigation(step['target'])\n        elif action_type == 'grasp_object':\n            self.execute_grasp(step['target'])\n        elif action_type == 'release_object':\n            self.execute_release(step['target'])\n        elif action_type == 'look_at':\n            self.execute_look(step['target'])\n        else:\n            self.get_logger().error(f'Unknown action type: {action_type}')\n            self.next_step()\n\n    def execute_navigation(self, target):\n        \"\"\"Execute navigation action\"\"\"\n        # Create navigation goal\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.header.frame_id = 'map'\n        goal_msg.pose.header.stamp = self.get_clock().now().to_msg()\n\n        # Convert target to pose (simplified)\n        # In practice, this would involve more complex mapping\n        goal_msg.pose.pose.position.x = 1.0  # Example coordinates\n        goal_msg.pose.pose.position.y = 1.0\n        goal_msg.pose.pose.orientation.w = 1.0\n\n        # Send navigation goal\n        self.nav_client.wait_for_server()\n        future = self.nav_client.send_goal_async(goal_msg)\n        future.add_done_callback(self.navigation_done_callback)\n\n    def navigation_done_callback(self, future):\n        \"\"\"Callback when navigation is completed\"\"\"\n        goal_handle = future.result()\n        if goal_handle.status == GoalStatus.STATUS_SUCCEEDED:\n            self.get_logger().info('Navigation succeeded')\n        else:\n            self.get_logger().info('Navigation failed')\n\n        self.next_step()\n\n    def next_step(self):\n        \"\"\"Move to the next step in the plan\"\"\"\n        self.current_step += 1\n        if self.current_step < len(self.current_plan):\n            # Execute next step\n            self.execute_current_step()\n        else:\n            # Plan completed\n            self.plan_completed()\n\n    def plan_completed(self):\n        \"\"\"Handle plan completion\"\"\"\n        self.is_executing = False\n        self.get_logger().info('Plan completed successfully')\n\n        # Publish completion status\n        status_msg = String()\n        status_msg.data = 'plan_completed'\n        self.status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    planning_node = PlanningNode()\n\n    try:\n        rclpy.spin(planning_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        planning_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,o.jsx)(n.h2,{id:"145-voice-command-executor",children:"14.5 Voice Command Executor"}),"\n",(0,o.jsx)(n.h3,{id:"1451-ros-2-action-server",children:"14.5.1 ROS 2 Action Server"}),"\n",(0,o.jsx)(n.p,{children:"To handle voice commands in a structured way, we can create an action server:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.action import ActionServer, GoalResponse, CancelResponse\nfrom rclpy.node import Node\nfrom rclpy.executors import MultiThreadedExecutor\nfrom rclpy.callback_groups import ReentrantCallbackGroup\n\nfrom std_msgs.msg import String\nfrom action_tutorials_interfaces.action import ExecuteVoiceCommand  # Custom action definition\n\nclass VoiceCommandExecutor(Node):\n    def __init__(self):\n        super().__init__(\'voice_command_executor\')\n\n        # Initialize components\n        self.nlp_processor = NaturalLanguageProcessor()\n        self.semantic_parser = SemanticParser()\n        self.task_planner = TaskPlanner()\n\n        # Create action server\n        self._action_server = ActionServer(\n            self,\n            ExecuteVoiceCommand,\n            \'execute_voice_command\',\n            execute_callback=self.execute_callback,\n            callback_group=ReentrantCallbackGroup(),\n            goal_callback=self.goal_callback,\n            cancel_callback=self.cancel_callback\n        )\n\n        # Publishers for status updates\n        self.status_pub = self.create_publisher(String, \'voice_command_status\', 10)\n\n        self.get_logger().info(\'Voice Command Executor ready\')\n\n    def goal_callback(self, goal_request):\n        """Accept or reject goal requests"""\n        self.get_logger().info(f\'Received voice command: {goal_request.command}\')\n        return GoalResponse.ACCEPT\n\n    def cancel_callback(self, goal_handle):\n        """Accept or reject cancel requests"""\n        self.get_logger().info(\'Received cancel request\')\n        return CancelResponse.ACCEPT\n\n    async def execute_callback(self, goal_handle):\n        """Execute the requested voice command"""\n        self.get_logger().info(\'Executing voice command...\')\n\n        feedback_msg = ExecuteVoiceCommand.Feedback()\n        result = ExecuteVoiceCommand.Result()\n\n        try:\n            # Process the command through the pipeline\n            command_text = goal_handle.request.command\n\n            # NLP processing\n            nlp_result = self.nlp_processor.process_command(command_text)\n            feedback_msg.status = f\'Processed NLP: intent={nlp_result["intent"]}\'\n            goal_handle.publish_feedback(feedback_msg)\n\n            # Semantic parsing\n            semantic_action = self.semantic_parser.parse_command(command_text, nlp_result[\'entities\'])\n            feedback_msg.status = f\'Parsed semantics: {semantic_action["action_type"]}\'\n            goal_handle.publish_feedback(feedback_msg)\n\n            # Planning\n            plan = self.task_planner.plan_task(semantic_action)\n            feedback_msg.status = f\'Generated plan with {len(plan)} steps\'\n            goal_handle.publish_feedback(feedback_msg)\n\n            # Execute plan\n            success = await self.execute_plan_async(plan, goal_handle, feedback_msg)\n\n            if success:\n                result.success = True\n                result.message = \'Command executed successfully\'\n                goal_handle.succeed()\n            else:\n                result.success = False\n                result.message = \'Command execution failed\'\n                goal_handle.abort()\n\n        except Exception as e:\n            self.get_logger().error(f\'Error executing command: {e}\')\n            result.success = False\n            result.message = f\'Execution error: {str(e)}\'\n            goal_handle.abort()\n\n        return result\n\n    async def execute_plan_async(self, plan, goal_handle, feedback_msg):\n        """Execute plan asynchronously with feedback"""\n        for i, step in enumerate(plan):\n            if goal_handle.is_cancel_requested:\n                goal_handle.canceled()\n                return False\n\n            feedback_msg.status = f\'Executing step {i+1}/{len(plan)}: {step["description"]}\'\n            goal_handle.publish_feedback(feedback_msg)\n\n            # Execute the step (simplified)\n            step_success = await self.execute_step_async(step)\n\n            if not step_success:\n                return False\n\n        return True\n\n    async def execute_step_async(self, step):\n        """Execute a single step of the plan"""\n        # This would involve calling appropriate ROS services/actions\n        # based on the step type\n        import asyncio\n        await asyncio.sleep(0.1)  # Simulate execution time\n        return True  # Simplified success\n\ndef main(args=None):\n    rclpy.init(args=args)\n    executor = VoiceCommandExecutor()\n\n    try:\n        rclpy.spin(executor)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        executor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h2,{id:"146-safety-and-error-handling",children:"14.6 Safety and Error Handling"}),"\n",(0,o.jsx)(n.h3,{id:"1461-command-validation",children:"14.6.1 Command Validation"}),"\n",(0,o.jsx)(n.p,{children:"Before executing any command, validate it for safety:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"class CommandValidator:\n    def __init__(self):\n        # Define safe zones and forbidden actions\n        self.safe_zones = [\n            # Define safe navigation areas\n        ]\n\n        self.forbidden_actions = [\n            'destroy',\n            'break',\n            'harm',\n            'dangerous'\n        ]\n\n    def validate_command(self, command, entities, current_robot_state):\n        \"\"\"Validate command for safety\"\"\"\n        command_lower = command.lower()\n\n        # Check for forbidden words\n        for forbidden in self.forbidden_actions:\n            if forbidden in command_lower:\n                return False, f'Command contains forbidden action: {forbidden}'\n\n        # Check navigation safety\n        if entities['locations']:\n            for location in entities['locations']:\n                if not self.is_safe_navigation_target(location, current_robot_state):\n                    return False, f'Navigation to {location} is unsafe'\n\n        # Check manipulation safety\n        if entities['objects']:\n            for obj in entities['objects']:\n                if not self.is_safe_to_manipulate(obj, current_robot_state):\n                    return False, f'Manipulation of {obj} is unsafe'\n\n        return True, 'Command is safe to execute'\n\n    def is_safe_navigation_target(self, location, robot_state):\n        \"\"\"Check if navigation to location is safe\"\"\"\n        # Implementation would check for obstacles, drop-offs, etc.\n        return True  # Simplified\n\n    def is_safe_to_manipulate(self, obj, robot_state):\n        \"\"\"Check if manipulation of object is safe\"\"\"\n        # Implementation would check object properties, robot state, etc.\n        return True  # Simplified\n"})}),"\n",(0,o.jsx)(n.h3,{id:"1462-error-recovery",children:"14.6.2 Error Recovery"}),"\n",(0,o.jsx)(n.p,{children:"Implement error recovery mechanisms:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'class ErrorRecovery:\n    def __init__(self):\n        self.recovery_strategies = {\n            \'navigation_failure\': self.recover_navigation,\n            \'grasp_failure\': self.recover_grasp,\n            \'perception_failure\': self.recover_perception\n        }\n\n    def handle_error(self, error_type, context):\n        """Handle different types of errors"""\n        if error_type in self.recovery_strategies:\n            return self.recovery_strategies[error_type](context)\n        else:\n            return self.default_recovery(context)\n\n    def recover_navigation(self, context):\n        """Recovery strategy for navigation failures"""\n        # Try alternative path\n        # Report to user\n        # Ask for clarification\n        return \'alternative_path_attempted\'\n\n    def recover_grasp(self, context):\n        """Recovery strategy for grasp failures"""\n        # Try different grasp approach\n        # Re-identify object\n        # Ask for help\n        return \'grasp_retry_attempted\'\n\n    def default_recovery(self, context):\n        """Default recovery for unknown errors"""\n        # Stop robot\n        # Report error\n        # Wait for human intervention\n        return \'stopped_for_intervention\'\n'})}),"\n",(0,o.jsx)(n.h2,{id:"147-performance-optimization",children:"14.7 Performance Optimization"}),"\n",(0,o.jsx)(n.h3,{id:"1471-real-time-processing",children:"14.7.1 Real-time Processing"}),"\n",(0,o.jsx)(n.p,{children:"For real-time voice control, optimize for latency:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import asyncio\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass OptimizedVoiceProcessor:\n    def __init__(self):\n        self.executor = ThreadPoolExecutor(max_workers=4)\n        self.loop = asyncio.get_event_loop()\n\n    async def process_voice_command(self, audio_data):\n        """Process voice command asynchronously"""\n        # Convert audio to text\n        text = await self.loop.run_in_executor(\n            self.executor,\n            self.speech_to_text,\n            audio_data\n        )\n\n        # Process NLP\n        nlp_result = await self.loop.run_in_executor(\n            self.executor,\n            self.nlp_process,\n            text\n        )\n\n        # Plan and execute\n        plan = await self.loop.run_in_executor(\n            self.executor,\n            self.plan_task,\n            nlp_result\n        )\n\n        return plan\n\n    def speech_to_text(self, audio_data):\n        """Convert audio to text (simplified)"""\n        # Implementation would use Whisper or similar\n        return "dummy text"\n\n    def nlp_process(self, text):\n        """Process text with NLP (simplified)"""\n        return {\'intent\': \'dummy\', \'entities\': {}}\n\n    def plan_task(self, nlp_result):\n        """Plan the task (simplified)"""\n        return [{\'action\': \'dummy\', \'target\': \'dummy\'}]\n'})}),"\n",(0,o.jsx)(n.h2,{id:"148-summary",children:"14.8 Summary"}),"\n",(0,o.jsx)(n.p,{children:"The voice \u2192 plan \u2192 action pipeline enables natural human-robot interaction by converting spoken commands into executable robot actions. This involves sophisticated processing at each stage: speech recognition to convert audio to text, natural language understanding to extract intent and entities, task planning to decompose high-level goals into executable actions, and safe execution with error recovery. The integration with ROS 2 provides a robust framework for implementing these systems on real robots."}),"\n",(0,o.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsx)(n.li,{children:"Implement a complete voice-controlled navigation system using the components described in this chapter."}),"\n",(0,o.jsx)(n.li,{children:"Extend the natural language processor to handle more complex commands with multiple objects and locations."}),"\n",(0,o.jsx)(n.li,{children:"Design and implement a safety validation system for voice commands."}),"\n",(0,o.jsx)(n.li,{children:"Evaluate the performance of your voice-controlled system with various users and command types."}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsx)(n.li,{children:'Radford, A., et al. (2022). "Robust Speech Recognition via Large-Scale Weak Supervision" (Whisper paper).'}),"\n",(0,o.jsx)(n.li,{children:'Kress-Gazit, H., et al. (2018). "Robotics and Automation for the Home".'}),"\n",(0,o.jsx)(n.li,{children:'Tellex, S., et al. (2011). "Understanding Natural Language Commands for Robotic Navigation".'}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>r});var o=t(6540);const i={},a=o.createContext(i);function s(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);